<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>

<link href='https://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext' rel='stylesheet' type='text/css' /><style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color: #ffffff; --text-color: #333333; --select-text-bg-color: #B5D6FC; --select-text-font-color: auto; --monospace: "Lucida Console",Consolas,"Courier",monospace; --title-bar-height: 20px; }
.mac-os-11 { --title-bar-height: 28px; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
h1, h2, h3, h4, h5 { white-space: pre-wrap; }
body { margin: 0px; padding: 0px; height: auto; inset: 0px; font-size: 1rem; line-height: 1.42857; overflow-x: hidden; background: inherit; }
iframe { margin: auto; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; overflow-wrap: break-word; position: relative; white-space: normal; overflow-x: visible; padding-top: 36px; }
#write.first-line-indent p { text-indent: 2em; }
#write.first-line-indent li p, #write.first-line-indent p * { text-indent: 0px; }
#write.first-line-indent li { margin-left: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
.typora-export .footnote-line, .typora-export li, .typora-export p { white-space: pre-wrap; }
.typora-export .task-list-item input { pointer-events: none; }
@media screen and (max-width: 500px) {
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  #write { padding-left: 20px; padding-right: 20px; }
}
#write li > figure:last-child { margin-bottom: 0.5rem; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; image-orientation: from-image; }
button, input, select, textarea { color: inherit; font: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
p { line-height: inherit; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 4; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
.md-math-block, .md-rawblock, h1, h2, h3, h4, h5, h6, p { margin-top: 1rem; margin-bottom: 1rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.7); color: rgb(85, 85, 85); border-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px; }
thead, tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 32px; }
.CodeMirror-gutters { border-right: 0px; background-color: inherit; }
.CodeMirror-linenumber { user-select: none; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
#write pre { white-space: pre-wrap; }
#write.fences-no-line-wrapping pre { white-space: pre; }
#write pre.ty-contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background: inherit; position: relative !important; }
.md-fences-adv-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
#write .md-fences.mock-cm { white-space: pre-wrap; }
.md-fences.md-fences-with-lineno { padding-left: 0px; }
#write.fences-no-line-wrapping .md-fences.mock-cm { white-space: pre; overflow-x: auto; }
.md-fences.mock-cm.md-fences-with-lineno { padding-left: 8px; }
.CodeMirror-line, twitterwidget { break-inside: avoid; }
svg { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; background: 0px 0px; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; -webkit-tap-highlight-color: transparent; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li blockquote { margin: 1rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
#write .footnote-line { white-space: pre-wrap; }
@media print {
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid; break-before: avoid; font-variant-ligatures: no-common-ligatures; }
  #write { margin-top: 0px; border-color: transparent !important; padding-top: 0px !important; padding-bottom: 0px !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  .typora-export #write { break-after: avoid; }
  .typora-export #write::after { height: 0px; }
  .is-mac table { break-inside: avoid; }
  #write > p:nth-child(1) { margin-top: 0px; }
  .typora-export-show-outline .typora-export-sidebar { display: none; }
  figure { overflow-x: visible; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background: rgb(204, 204, 204); display: block; overflow-x: hidden; }
p > .md-image:only-child:not(.md-img-error) img, p > img:only-child { display: block; margin: auto; }
#write.first-line-indent p > .md-image:only-child:not(.md-img-error) img { left: -2em; position: relative; }
p > .md-image:only-child { display: inline-block; width: 100%; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.md-math-block { width: 100%; }
.md-math-block:not(:empty)::after { display: none; }
.MathJax_ref { fill: currentcolor; }
[contenteditable="true"]:active, [contenteditable="true"]:focus, [contenteditable="false"]:active, [contenteditable="false"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); border: none; }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { text-decoration: underline; }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) {
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.reversefootnote { font-family: ui-monospace, sans-serif; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, samp, tt { font-family: var(--monospace); }
kbd { margin: 0px 0.1em; padding: 0.1em 0.6em; font-size: 0.8em; color: rgb(36, 39, 41); background: rgb(255, 255, 255); border: 1px solid rgb(173, 179, 185); border-radius: 3px; box-shadow: rgba(12, 13, 14, 0.2) 0px 1px 0px, rgb(255, 255, 255) 0px 0px 0px 2px inset; white-space: nowrap; vertical-align: middle; }
.md-comment { color: rgb(162, 127, 3); opacity: 0.6; font-family: var(--monospace); }
code { text-align: left; vertical-align: initial; }
a.md-print-anchor { white-space: pre !important; border-width: initial !important; border-style: none !important; border-color: initial !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; background: 0px 0px !important; text-decoration: initial !important; text-shadow: initial !important; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="flow"] svg, [lang="mermaid"] svg { max-width: 100%; height: auto; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom: 0px; }
video { max-width: 100%; display: block; margin: 0px auto; }
iframe { max-width: 100%; width: 100%; border: none; }
.highlight td, .highlight tr { border: 0px; }
mark { background: rgb(255, 255, 0); color: rgb(0, 0, 0); }
.md-html-inline .md-plain, .md-html-inline strong, mark .md-inline-math, mark strong { color: inherit; }
.md-expand mark .md-meta { opacity: 0.3 !important; }
mark .md-meta { color: rgb(0, 0, 0); }
@media print {
  .typora-export h1, .typora-export h2, .typora-export h3, .typora-export h4, .typora-export h5, .typora-export h6 { break-inside: avoid; }
}
.md-diagram-panel .messageText { stroke: none !important; }
.md-diagram-panel .start-state { fill: var(--node-fill); }
.md-diagram-panel .edgeLabel rect { opacity: 1 !important; }
.md-fences.md-fences-math { font-size: 1em; }
.md-fences-advanced:not(.md-focus) { padding: 0px; white-space: nowrap; border: 0px; }
.md-fences-advanced:not(.md-focus) { background: inherit; }
.typora-export-show-outline .typora-export-content { max-width: 1440px; margin: auto; display: flex; flex-direction: row; }
.typora-export-sidebar { width: 300px; font-size: 0.8rem; margin-top: 80px; margin-right: 18px; }
.typora-export-show-outline #write { --webkit-flex: 2; flex: 2 1 0%; }
.typora-export-sidebar .outline-content { position: fixed; top: 0px; max-height: 100%; overflow: hidden auto; padding-bottom: 30px; padding-top: 60px; width: 300px; }
@media screen and (max-width: 1024px) {
  .typora-export-sidebar, .typora-export-sidebar .outline-content { width: 240px; }
}
@media screen and (max-width: 800px) {
  .typora-export-sidebar { display: none; }
}
.outline-content li, .outline-content ul { margin-left: 0px; margin-right: 0px; padding-left: 0px; padding-right: 0px; list-style: none; overflow-wrap: anywhere; }
.outline-content ul { margin-top: 0px; margin-bottom: 0px; }
.outline-content strong { font-weight: 400; }
.outline-expander { width: 1rem; height: 1.42857rem; position: relative; display: table-cell; vertical-align: middle; cursor: pointer; padding-left: 4px; }
.outline-expander::before { content: ""; position: relative; font-family: Ionicons; display: inline-block; font-size: 8px; vertical-align: middle; }
.outline-item { padding-top: 3px; padding-bottom: 3px; cursor: pointer; }
.outline-expander:hover::before { content: ""; }
.outline-h1 > .outline-item { padding-left: 0px; }
.outline-h2 > .outline-item { padding-left: 1em; }
.outline-h3 > .outline-item { padding-left: 2em; }
.outline-h4 > .outline-item { padding-left: 3em; }
.outline-h5 > .outline-item { padding-left: 4em; }
.outline-h6 > .outline-item { padding-left: 5em; }
.outline-label { cursor: pointer; display: table-cell; vertical-align: middle; text-decoration: none; color: inherit; }
.outline-label:hover { text-decoration: underline; }
.outline-item:hover { border-color: rgb(245, 245, 245); background-color: var(--item-hover-bg-color); }
.outline-item:hover { margin-left: -28px; margin-right: -28px; border-left: 28px solid transparent; border-right: 28px solid transparent; }
.outline-item-single .outline-expander::before, .outline-item-single .outline-expander:hover::before { display: none; }
.outline-item-open > .outline-item > .outline-expander::before { content: ""; }
.outline-children { display: none; }
.info-panel-tab-wrapper { display: none; }
.outline-item-open > .outline-children { display: block; }
.typora-export .outline-item { padding-top: 1px; padding-bottom: 1px; }
.typora-export .outline-item:hover { margin-right: -8px; border-right: 8px solid transparent; }
.typora-export .outline-expander::before { content: "+"; font-family: inherit; top: -1px; }
.typora-export .outline-expander:hover::before, .typora-export .outline-item-open > .outline-item > .outline-expander::before { content: "−"; }
.typora-export-collapse-outline .outline-children { display: none; }
.typora-export-collapse-outline .outline-item-open > .outline-children, .typora-export-no-collapse-outline .outline-children { display: block; }
.typora-export-no-collapse-outline .outline-expander::before { content: "" !important; }
.typora-export-show-outline .outline-item-active > .outline-item .outline-label { font-weight: 700; }
.md-inline-math-container mjx-container { zoom: 0.95; }
mjx-container { break-inside: avoid; }
.md-alert.md-alert-note { border-left-color: rgb(9, 105, 218); }
.md-alert.md-alert-important { border-left-color: rgb(130, 80, 223); }
.md-alert.md-alert-warning { border-left-color: rgb(154, 103, 0); }
.md-alert.md-alert-tip { border-left-color: rgb(31, 136, 61); }
.md-alert.md-alert-caution { border-left-color: rgb(207, 34, 46); }
.md-alert { padding: 0px 1em; margin-bottom: 16px; color: inherit; border-left: 0.25em solid rgb(0, 0, 0); }
.md-alert-text-note { color: rgb(9, 105, 218); }
.md-alert-text-important { color: rgb(130, 80, 223); }
.md-alert-text-warning { color: rgb(154, 103, 0); }
.md-alert-text-tip { color: rgb(31, 136, 61); }
.md-alert-text-caution { color: rgb(207, 34, 46); }
.md-alert-text { font-size: 0.9rem; font-weight: 700; }
.md-alert-text svg { fill: currentcolor; position: relative; top: 0.125em; margin-right: 1ch; overflow: visible; }
.md-alert-text-container::after { content: attr(data-text); text-transform: capitalize; pointer-events: none; margin-right: 1ch; }


:root {
    --side-bar-bg-color: #fafafa;
    --control-text-color: #777;
}

@include-when-export url(https://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext);

/* open-sans-regular - latin-ext_latin */
  /* open-sans-italic - latin-ext_latin */
    /* open-sans-700 - latin-ext_latin */
    /* open-sans-700italic - latin-ext_latin */
  html {
    font-size: 16px;
    -webkit-font-smoothing: antialiased;
}

body {
    font-family: "Open Sans","Clear Sans", "Helvetica Neue", Helvetica, Arial, 'Segoe UI Emoji', sans-serif;
    color: rgb(51, 51, 51);
    line-height: 1.6;
}

#write {
    max-width: 860px;
  	margin: 0 auto;
  	padding: 30px;
    padding-bottom: 100px;
}

@media only screen and (min-width: 1400px) {
	#write {
		max-width: 1024px;
	}
}

@media only screen and (min-width: 1800px) {
	#write {
		max-width: 1200px;
	}
}

#write > ul:first-child,
#write > ol:first-child{
    margin-top: 30px;
}

a {
    color: #4183C4;
}
h1,
h2,
h3,
h4,
h5,
h6 {
    position: relative;
    margin-top: 1rem;
    margin-bottom: 1rem;
    font-weight: bold;
    line-height: 1.4;
    cursor: text;
}
h1:hover a.anchor,
h2:hover a.anchor,
h3:hover a.anchor,
h4:hover a.anchor,
h5:hover a.anchor,
h6:hover a.anchor {
    text-decoration: none;
}
h1 tt,
h1 code {
    font-size: inherit;
}
h2 tt,
h2 code {
    font-size: inherit;
}
h3 tt,
h3 code {
    font-size: inherit;
}
h4 tt,
h4 code {
    font-size: inherit;
}
h5 tt,
h5 code {
    font-size: inherit;
}
h6 tt,
h6 code {
    font-size: inherit;
}
h1 {
    font-size: 2.25em;
    line-height: 1.2;
    border-bottom: 1px solid #eee;
}
h2 {
    font-size: 1.75em;
    line-height: 1.225;
    border-bottom: 1px solid #eee;
}

/*@media print {
    .typora-export h1,
    .typora-export h2 {
        border-bottom: none;
        padding-bottom: initial;
    }

    .typora-export h1::after,
    .typora-export h2::after {
        content: "";
        display: block;
        height: 100px;
        margin-top: -96px;
        border-top: 1px solid #eee;
    }
}*/

h3 {
    font-size: 1.5em;
    line-height: 1.43;
}
h4 {
    font-size: 1.25em;
}
h5 {
    font-size: 1em;
}
h6 {
   font-size: 1em;
    color: #777;
}
p,
blockquote,
ul,
ol,
dl,
table{
    margin: 0.8em 0;
}
li>ol,
li>ul {
    margin: 0 0;
}
hr {
    height: 2px;
    padding: 0;
    margin: 16px 0;
    background-color: #e7e7e7;
    border: 0 none;
    overflow: hidden;
    box-sizing: content-box;
}

li p.first {
    display: inline-block;
}
ul,
ol {
    padding-left: 30px;
}
ul:first-child,
ol:first-child {
    margin-top: 0;
}
ul:last-child,
ol:last-child {
    margin-bottom: 0;
}
blockquote {
    border-left: 4px solid #dfe2e5;
    padding: 0 15px;
    color: #777777;
}
blockquote blockquote {
    padding-right: 0;
}
table {
    padding: 0;
    word-break: initial;
}
table tr {
    border: 1px solid #dfe2e5;
    margin: 0;
    padding: 0;
}
table tr:nth-child(2n),
thead {
    background-color: #f8f8f8;
}
table th {
    font-weight: bold;
    border: 1px solid #dfe2e5;
    border-bottom: 0;
    margin: 0;
    padding: 6px 13px;
}
table td {
    border: 1px solid #dfe2e5;
    margin: 0;
    padding: 6px 13px;
}
table th:first-child,
table td:first-child {
    margin-top: 0;
}
table th:last-child,
table td:last-child {
    margin-bottom: 0;
}

.CodeMirror-lines {
    padding-left: 4px;
}

.code-tooltip {
    box-shadow: 0 1px 1px 0 rgba(0,28,36,.3);
    border-top: 1px solid #eef2f2;
}

.md-fences,
code,
tt {
    border: 1px solid #e7eaed;
    background-color: #f8f8f8;
    border-radius: 3px;
    padding: 0;
    padding: 2px 4px 0px 4px;
    font-size: 0.9em;
}

code {
    background-color: #f3f4f4;
    padding: 0 2px 0 2px;
}

.md-fences {
    margin-bottom: 15px;
    margin-top: 15px;
    padding-top: 8px;
    padding-bottom: 6px;
}


.md-task-list-item > input {
  margin-left: -1.3em;
}

@media print {
    html {
        font-size: 13px;
    }
    pre {
        page-break-inside: avoid;
        word-wrap: break-word;
    }
}

.md-fences {
	background-color: #f8f8f8;
}
#write pre.md-meta-block {
	padding: 1rem;
    font-size: 85%;
    line-height: 1.45;
    background-color: #f7f7f7;
    border: 0;
    border-radius: 3px;
    color: #777777;
    margin-top: 0 !important;
}

.mathjax-block>.code-tooltip {
	bottom: .375rem;
}

.md-mathjax-midline {
    background: #fafafa;
}

#write>h3.md-focus:before{
	left: -1.5625rem;
	top: .375rem;
}
#write>h4.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
#write>h5.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
#write>h6.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
.md-image>.md-meta {
    /*border: 1px solid #ddd;*/
    border-radius: 3px;
    padding: 2px 0px 0px 4px;
    font-size: 0.9em;
    color: inherit;
}

.md-tag {
    color: #a7a7a7;
    opacity: 1;
}

.md-toc { 
    margin-top:20px;
    padding-bottom:20px;
}

.sidebar-tabs {
    border-bottom: none;
}

#typora-quick-open {
    border: 1px solid #ddd;
    background-color: #f8f8f8;
}

#typora-quick-open-item {
    background-color: #FAFAFA;
    border-color: #FEFEFE #e5e5e5 #e5e5e5 #eee;
    border-style: solid;
    border-width: 1px;
}

/** focus mode */
.on-focus-mode blockquote {
    border-left-color: rgba(85, 85, 85, 0.12);
}

header, .context-menu, .megamenu-content, footer{
    font-family: "Segoe UI", "Arial", sans-serif;
}

.file-node-content:hover .file-node-icon,
.file-node-content:hover .file-node-open-state{
    visibility: visible;
}

.mac-seamless-mode #typora-sidebar {
    background-color: #fafafa;
    background-color: var(--side-bar-bg-color);
}

.mac-os #write{
    caret-color: AccentColor;
}

.md-lang {
    color: #b4654d;
}

/*.html-for-mac {
    --item-hover-bg-color: #E6F0FE;
}*/

#md-notification .btn {
    border: 0;
}

.dropdown-menu .divider {
    border-color: #e5e5e5;
    opacity: 0.4;
}

.ty-preferences .window-content {
    background-color: #fafafa;
}

.ty-preferences .nav-group-item.active {
    color: white;
    background: #999;
}

.menu-item-container a.menu-style-btn {
    background-color: #f5f8fa;
    background-image: linear-gradient( 180deg , hsla(0, 0%, 100%, 0.8), hsla(0, 0%, 100%, 0)); 
}



mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
  min-height: 1px;
  min-width: 1px;
}

mjx-container[jax="SVG"] > svg a {
  fill: blue;
  stroke: blue;
}

mjx-assistive-mml {
  position: absolute !important;
  top: 0px;
  left: 0px;
  clip: rect(1px, 1px, 1px, 1px);
  padding: 1px 0px 0px 0px !important;
  border: 0px !important;
  display: block !important;
  width: auto !important;
  overflow: hidden !important;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

mjx-assistive-mml[display="block"] {
  width: 100% !important;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][display="true"][width="full"] {
  display: flex;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line], svg[data-table] > g > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame], svg[data-table] > g > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed, svg[data-table] > g > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted, svg[data-table] > g > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > g > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

mjx-container[jax="SVG2"] path[data-c], mjx-container[jax="SVG2"] use[data-c] {
  stroke-width: 3;
}

g[data-mml-node="xypic"] path {
  stroke-width: inherit;
}

.MathJax g[data-mml-node="xypic"] path {
  stroke-width: inherit;
}
mjx-container[jax="SVG"] path[data-c], mjx-container[jax="SVG"] use[data-c] {
							stroke-width: 0;
						}
</style><title>AnyGPT</title>
</head>
<body class='typora-export os-windows'><div class='typora-export-content'>
<div id='write'  class=''><head>
<style>
    body {text-align: justify;}
</style>
</head><div class='md-toc' mdtype='toc'><p class="md-toc-content" role="list"><span role="listitem" class="md-toc-item md-toc-h1" data-ref="n744"><a class="md-toc-inner" href="#abstract">Abstract</a></span><span role="listitem" class="md-toc-item md-toc-h1" data-ref="n746"><a class="md-toc-inner" href="#1-introduction">1 Introduction</a></span><span role="listitem" class="md-toc-item md-toc-h1" data-ref="n759"><a class="md-toc-inner" href="#3-anygpt">3 AnyGPT</a></span><span role="listitem" class="md-toc-item md-toc-h2" data-ref="n764"><a class="md-toc-inner" href="#31-tokenization">3.1 Tokenization</a></span><span role="listitem" class="md-toc-item md-toc-h2" data-ref="n773"><a class="md-toc-inner" href="#32-language-model-backbone">3.2 Language Model Backbone</a></span><span role="listitem" class="md-toc-item md-toc-h2" data-ref="n777"><a class="md-toc-inner" href="#33-multimodal-generation">3.3 Multimodal Generation</a></span><span role="listitem" class="md-toc-item md-toc-h1" data-ref="n781"><a class="md-toc-inner" href="#4-multimodal-data">4 Multimodal Data</a></span><span role="listitem" class="md-toc-item md-toc-h2" data-ref="n782"><a class="md-toc-inner" href="#41-pre-training-data">4.1 Pre-training Data</a></span><span role="listitem" class="md-toc-item md-toc-h2" data-ref="n802"><a class="md-toc-inner" href="#42-multimodal-interleaved-instruction-data-construction-data">4.2 Multimodal Interleaved Instruction Data Construction Data</a></span><span role="listitem" class="md-toc-item md-toc-h1" data-ref="n811"><a class="md-toc-inner" href="#5-experiment">5 Experiment</a></span><span role="listitem" class="md-toc-item md-toc-h2" data-ref="n812"><a class="md-toc-inner" href="#51-evaluation">5.1 Evaluation</a></span><span role="listitem" class="md-toc-item md-toc-h3" data-ref="n815"><a class="md-toc-inner" href="#511-image">5.1.1 Image</a></span><span role="listitem" class="md-toc-item md-toc-h3" data-ref="n820"><a class="md-toc-inner" href="#512-speech">5.1.2 Speech</a></span><span role="listitem" class="md-toc-item md-toc-h2" data-ref="n831"><a class="md-toc-inner" href="#513-music">5.1.3 Music</a></span><span role="listitem" class="md-toc-item md-toc-h2" data-ref="n836"><a class="md-toc-inner" href="#52-example-demonstrations">5.2 Example Demonstrations</a></span><span role="listitem" class="md-toc-item md-toc-h1" data-ref="n838"><a class="md-toc-inner" href="#6-conclusion">6 Conclusion</a></span></p></div><p style="text-align: center; font-weight: bold; font-size: 24px;">
AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling
</p><p><strong><span>Title:</span></strong><span> AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling</span>
<strong><span>Paper:</span></strong><span> </span><a href='https://arxiv.org/abs/2402.12226' target='_blank' class='url'>https://arxiv.org/abs/2402.12226</a>
<strong><span>Project page:</span></strong><span> </span><a href='https://junzhan2000.github.io/AnyGPT.github.io/' target='_blank' class='url'>https://junzhan2000.github.io/AnyGPT.github.io/</a><span> </span>
<strong><span>Github:</span></strong><span> </span><a href='https://github.com/OpenMOSS/AnyGPT' target='_blank' class='url'>https://github.com/OpenMOSS/AnyGPT</a>
<strong><span>Zhihu:</span></strong><span> </span><a href='https://zhuanlan.zhihu.com/p/683058051' target='_blank' class='url'>https://zhuanlan.zhihu.com/p/683058051</a>
<strong><span>Authors:</span></strong><span>  Fudan University, Multimodal Art Projection Research Community, Shanghai AI Laboratory</span></p><h1 id='abstract'><font face="Times New Roman"><span>Abstract</span></font></h1><p><span>我们介绍了 AnyGPT，这是一种 any-to-any 的多模态语言模型，它使用离散表示来统一处理各种模态，包括语音、文本、图像和音乐。AnyGPT 可以稳定地训练，而不需要对当前的大型语言模型 (LLM) 体系结构或训练范例进行任何更改。相反，它完全依赖于数据级的预处理，便于将新的motai无缝地整合到 LLMs 中，类似于合并新的语言。我们构建了一个以文本为中心的多模态数据集，用于多模态对齐的预训练。利用生成式模型，我们合成了第一个大规模 any-to-any 的多模态指令数据集。它由 108K 个多轮对话样本组成，这些样本错综复杂地交织着各种模态，从而使该模型能够处理多模态输入和输出的任意组合。实验结果表明，AnyGPT 能够促进 any-to-any 多模态对话，同时在所有模态上获得与专用模型相当的性能，证明了离散表示可以有效且方便地将多个模态统一在一个语言模型中。</span></p><h1 id='1-introduction'><font face="Times New Roman"><span>1 Introduction</span></font></h1><p><span>现有的对任意多模态生成的探索遇到了阻碍：一些缺乏健壮的核心语言模型，这阻碍了系统的推理和决策能力；其他的，如 NExT-GPT ，CoDi-2 和 Unified-IO2 ，分别使用预训练的编码器和解码器。这种方法导致 LLMs 的输入和输出之间的表示不一致，这反过来又使训练和推理过程复杂化。此外，要以这种多样化的方式稳定训练，就必须对现有的模型和技术作出重大修改。</span><font color=RoyalBlue><span>(编码端的输出与解码端的输入不一样，如对于 image，由 ViT、Q-Former等编码为 token_img，而解码时需要 [IMGr]，并与 SD 的 text encoder对齐，即编码时所需要的 token_img 与解码时所需要的 [IMGr] 不是一回事)</span></font></p><p><span>为了克服这些挑战，我们引入了 AnyGPT，这是一种 any-to-any 的多模态语言模型，它采用离散表示进行统一处理。AnyGPT 配备了多模态 tokenizer，可以将原始的多模态数据 (如图像和音频) 压缩成一个离散的语义 tokens 的序列。这些离散表示使核心 LLM 能够在语义层以自回归的方式统一感知、理解、推理和生成等任务。随后，de-tokenizers 在感知层面将离散表征转换回原始模态表征。由于离散表示 (它过滤掉高频、模态特定的感知信息，同时保留必要的低频语义信息)，我们可以在不改变现有 LLM 架构或训练范式的情况下稳定地训练我们的模型。相反，我们的方法完全依赖于数据级预处理。这允许将新模态无缝集成到 LLMs 中，类似于添加新语言，并允许直接应用现有的 LLM 工具，从而提高训练和推理阶段的效率。</span></p><p><span>此外，为了缓解包含所有模态的多模态对齐数据的稀缺性，我们构建了一个以文本为中心的多模态对齐数据集用于预训练。我们的目标是使用文本作为桥梁，通过将其他模态与文本对齐，实现所有模态之间的相互对齐，因为自然语言是语义表示中最精细的模态，并且存在于大多数多模态对齐数据集中。</span></p><p><span>为了赋予该模型理解和生成多模态交织内容的能力，我们采用先进的生成模型来合成一个多模态指令数据集 AnyInstruct-108k。该数据集包含 108k 个多轮对话样本，使 AnyGPT 能够处理多模态输入和输出的任意组合。</span></p><p><span>我们的贡献包括:</span></p><ul><li><p><span>我们提出了 AnyGPT，一个基于 token 的 any-to-any 多模态语言模型，可以理解和生成各种模态，包括语音、文本、图像和音乐。</span></p></li><li><p><span>一个关键的挑战是缺乏多模态交错指令遵循的数据。我们开发了一个使用生成模型的 pipeline 来构建 AnyInstruct-108k，这是一个包含 108k 多轮对话的数据集，具有交错的多模态元素。</span></p></li><li><p><span>我们证明离散表示可以有效地统一语言模型中的多个模态。</span></p></li></ul><h1 id='3-anygpt'><font face="Times New Roman"><span>3 AnyGPT</span></font></h1><div class="columns is-centered" id="figure_1">
<center><img src="figure_1.png" width="90%"></center>
</div><p><span>我们的兴趣在于促进与 LLMs 任意模态到任意模态 的生成。为了实现这一点，我们提出了一个可以统一训练的综合框架。如图1所示，该框架由三个主要组件组成：(1) 多模态 tokenizers，(2) 作为 backbone 的多模态语言模型，以及 (3) 多模态 de-tokenizers。Tokenizers 将连续的非文本模态转换为离散的 tokens，这些 tokens 随后排列成多模态交错序列。然后，语言模型使用 next token prediction 的训练目标对序列进行训练。在推理过程中，多模态 tokens 被相关的 de-tokenizers 解码回其原始表示。为了丰富生成的质量，可以部署多模态增强模块对生成的结果进行后处理，包括语音克隆或图像超分辨率等应用。在下一节中，我们将介绍每个模块的详细信息。</span></p><div class="columns is-centered" id="figure_4">
<center><img src="figure_4.png" width="90%"></center>
</div><h2 id='31-tokenization'><font face="Times New Roman"><span>3.1 Tokenization</span></font></h2><div class="columns is-centered" id="table_1">
<center><img src="table_1.png" width="40%"></center>
</div><p><strong><span>Image Tokenizer</span></strong><span> 我们使用 </span><a href='https://arxiv.org/abs/2307.08041'><span>SEED tokenizer</span></a><span> 进行图像 tokenization。SEED tokenizer 由几个组件组成，包括 ViT encoder、Causal Q-Former、VQ Codebook、多层感知机 (MLP) 和 UNet decoder。SEED 以一张 224 × 224 RGB 图像作为输入，ViT encoder 将图像编码成 16 × 16 个 patch，然后 Causal Q-Former 将patch 特征转换成 32 个 causal embeddings。一个有 8192 个条目的 codebook 将 embeddings 离散成一个量化 code 序列。使用 MLP 将视觉 codes 解码为生成 embedding，该 embedding 与预训练的 unCLIP Stable Diffusion (unCLIP- SD) 的潜在空间一致。最后，使用UNet decoder 将生成 embedding 恢复到原始图像。</span></p><blockquote><p><span>对于SEED，有tokenizer，也有相应的de-tokenizer。通过 tokenizer 将图像编码成相应的 token，通过de-tokenizer 是可以直接还原成图像的。</span>
<span>而NExT-GPT等模型却是：图像编码后的表征只能在感知时用，而生成图像又是另一种方法。</span></p></blockquote><p><strong><span>Speech Tokenizer</span></strong><span> 我们使用的语音 tokenizer 是 </span><a href='https://arxiv.org/abs/2308.16692'><span>SpeechTokenizer</span></a><span>，采用残差矢量量化(residual vector quantization, RVQ) 的 encoder-decoder 架构。SpeechTokenizer 使用八个分层量化器将单通道音频序列压缩成离散矩阵，每个量化器有 1024 个条目，并实现 50 Hz 的帧率。第一个量化层捕获语义内容，而第 2 层到第 8 层编码副语言细节。因此，一个 10 秒的音频被转换成一个 500 × 8 的矩阵，分为语义和声学 tokens。我们采用了在 Commonvoice 和LibriSpeech 数据集上预训练的 SpeechTokenizer 的变体。</span></p><p>&emsp;&emsp;<span>在AnyGPT中，使用 LLM 对语义 tokens 进行建模，而语音克隆模型补充了剩余的副语言信息。因此，LLM 中语音词汇表的大小相当于一个 codebook 的大小，即 1024。进一步的细节将在第 3.3 节讨论。</span></p><p><strong><span>Music Tokenizer</span></strong><span> 虽然语音和音乐共享相似的数据格式，但它们的实质内容差异导致我们将它们视为不同的模态，每种模态都配备了自己的 tokenizer。对于音乐，我们使用 </span><a href='https://arxiv.org/abs/2210.13438'><span>Encodec</span></a><span> 作为音乐 tokenizer，Encodec 是一种卷积自编码器，使用残量矢量量化 (RVQ) 对潜在空间进行量化。我们使用了一个现成的 </span><a href='https://huggingface.co/facebook/encodec_32khz'><span>Encodec 变体</span></a><span>，它在 20k 个音乐曲目上进行了预训练。该变体处理 32 kHz 单声道音频，并实现 50 Hz 的帧率。生成的 embeddings 使用带有四个量化器的 RVQ 进行量化，每个量化器的 codebook 大小为 2048，从而得到 8192 个音乐词汇表的组合大小。</span></p><p>&emsp;&emsp;<span>我们将 5 秒的音乐编码成 250 个潜在帧，最终生成一个 250×4 的编码矩阵。为了使语言模型能够预测整个音乐片段，我们以逐帧的方式将 4 层音乐 codes 平铺成因果序列 (causal sequence)。语言模型首先预测第一帧的初始四个 tokens，然后以类似的方式继续预测后续帧。</span></p><h2 id='32-language-model-backbone'><font face="Times New Roman"><span>3.2 Language Model Backbone</span></font></h2><p><strong><span>Expanding vocabulary</span></strong><span> 为了将多模态离散表示合并到预训练的 LLMs 中，我们使用新的 modality-specific tokens 扩展词汇表，从而扩展相应的 embeddings 和 prediction layer，新合并的参数随机初始化。来自所有模态的tokens组合起来形成一个新的词汇表，其中每个模态在语言模型中进行训练，以便在共享的表示空间中对齐。这个增强的词汇量的大小，用 </span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.05ex;"><defs><path id="MJX-8-TEX-I-1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D449" xlink:href="#MJX-8-TEX-I-1D449"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>V</mi></math></mjx-assistive-mml></mjx-container><script type="math/tex">V</script><span> 表示，是所有模态词汇量的总和，即 </span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="12.366ex" height="2.563ex" role="img" focusable="false" viewBox="0 -789.6 5465.8 1132.9" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.777ex;"><defs><path id="MJX-9-TEX-I-1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path><path id="MJX-9-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-9-TEX-SO-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path><path id="MJX-9-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-9-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-9-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D449" xlink:href="#MJX-9-TEX-I-1D449"></use></g><g data-mml-node="mo" transform="translate(1046.8,0)"><use data-c="3D" xlink:href="#MJX-9-TEX-N-3D"></use></g><g data-mml-node="munderover" transform="translate(2102.6,0)"><g data-mml-node="mo"><use data-c="2211" xlink:href="#MJX-9-TEX-SO-2211"></use></g><g data-mml-node="mi" transform="translate(1089,477.1) scale(0.707)"><use data-c="1D45B" xlink:href="#MJX-9-TEX-I-1D45B"></use></g><g data-mml-node="TeXAtom" transform="translate(1089,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-9-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345,0)"><use data-c="3D" xlink:href="#MJX-9-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1123,0)"><use data-c="31" xlink:href="#MJX-9-TEX-N-31"></use></g></g></g><g data-mml-node="msub" transform="translate(4555.9,0)"><g data-mml-node="mi"><use data-c="1D449" xlink:href="#MJX-9-TEX-I-1D449"></use></g><g data-mml-node="mi" transform="translate(616,-150) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-9-TEX-I-1D456"></use></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>V</mi><mo>=</mo><munderover><mo data-mjx-texclass="OP">∑</mo><mrow data-mjx-texclass="ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>V</mi><mi>i</mi></msub></math></mjx-assistive-mml></mjx-container><script type="math/tex">V = \sum_{i=1}^n V_i</script><span>，其中 </span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="2.059ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 910 840.8" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.357ex;"><defs><path id="MJX-10-TEX-I-1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path><path id="MJX-10-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D449" xlink:href="#MJX-10-TEX-I-1D449"></use></g><g data-mml-node="mi" transform="translate(616,-150) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-10-TEX-I-1D456"></use></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>V</mi><mi>i</mi></msub></math></mjx-assistive-mml></mjx-container><script type="math/tex">V_i</script><span> 表示第 </span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.025ex;"><defs><path id="MJX-11-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-11-TEX-I-1D456"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>i</mi></math></mjx-assistive-mml></mjx-container><script type="math/tex">i</script><span> 个模态的词汇量。</span></p><p><strong><span>Unified Multimodal Language Model</span></strong><span> 配备 modality-specific tokenizer ，我们可以将多模态数据压缩成离散的 token 序列，这些序列可以通过语言模型使用 next token prediction loss 进行训练。这特征使核心 LLM 能够以自回归的方式统一感知、理解、推理和生成等任务。</span></p><p>&emsp;&emsp;<span>我们使用 LLaMA-2 7B 作为 backbone，它在 2TB 的文本 tokens 上进行预训练。除了重塑 embedding 矩阵和预测层外，语言模型的其余部分保持不变。</span></p><h2 id='33-multimodal-generation'><font face="Times New Roman"><span>3.3 Multimodal Generation</span></font></h2><p><span>生成高质量的多模态数据，包括高清图像和高保真音频，面向一个实质性的挑战。这些数据通常需要大量的 bits 来精确表示，导致长序列，这对语言模型的要求特别高，因为计算复杂性随着序列的长度呈指数增长。</span></p><p><span>为了解决这个问题，我们采用了一个两阶段的高保真生成框架，包括语义信息建模和感知信息建模。首先，语言模型的任务是生成在语义层进行融合和对齐的内容。然后，非自回归模型在感知层面将多模态语义 tokens 转换为高保真的多模态内容，在性能和效率之间取得平衡。</span></p><p><span>具体来说，我们使用 SEED tokens，与扩散潜在空间 (diffusion latent space) 对齐，用于视觉语言建模。语义级 SEED tokens 通过扩散模型解码成高质量的图像，扩散模型以其优越的生成能力而闻名。对于语音，我们使用 </span><a href='https://arxiv.org/abs/2305.09636'><span>SoundStorm</span></a><span>，这是一种非自回归的掩码语言模型，经过训练可以从语义 tokens 生成语音 tokenizer 的声学 tokens。我们训练了一个 Soundstorm 的变体，它是使用Multilingual LibriSpeech (MLS) 数据集上的 SpeechTokenizer 进行训练的。随后，SpeechTokenizer 的解码器将所有语音 tokens 转换为原始音频数据。这种方法使 AnyGPT 能够使用 3 秒的语音提示复制任何说话者的声音，同时显着减少了 LLM 语音序列的长度。对于音乐，我们使用编解码器 tokens 来过滤掉超出人类感知的高频细节，然后使用编解码器将这些 tokens 重构为高保真音频数据。</span></p><h1 id='4-multimodal-data'><font face="Times New Roman"><span>4 Multimodal Data</span></font></h1><h2 id='41-pre-training-data'><font face="Times New Roman"><span>4.1 Pre-training Data</span></font></h2><p><span>为了实现从任意模态到任意其他模态的生成，在这些模态之间具有良好对齐的数据是至关重要的。不幸的是，这样的数据非常稀少。为了应对这一挑战，我们构建了一个以文本为中心的双模态对齐数据集。在这里，文本被用作一个重要的中介，以弥合各种形式之间的差距。通过将不同的模态与语言模型中的文本模态对齐，我们的目标是实现所有模态之间的相互对齐。</span></p><div class="columns is-centered" id="figure_2">
<center><img src="figure_2.png" width="40%"></center>
</div><p><span>信息的表示形式和类型在不同的模态下差异很大。为了促进不同模式下数据量的标准化比较，我们采用了基于 token 计数的量化方法。Figure 2 给出了预训练中使用的所有数据以及它们各自的比例。一定程度的过采样应用于数据量相对较低的模式，以在单个批次中获得不同数据类型的平衡表示。</span><em><span>更多细节见 Appendix A.1。</span></em></p><blockquote><p><strong><font face="Times New Roman"><span>A.1 Data</span></font></strong></p><div class="columns is-centered" id="table_7">
<center><img src="table_7.png" width="90%"></center>
</div></blockquote><p><strong><span>Image &amp; Text</span></strong><span> 我们使用了来自LAION-2B 、LAION-COCO、LAION-Aesthetics 和 JouneyDB 的图像-文本对。LAION-2B 提供了与来自网络的嘈杂的替代文本配对的图像，而 LAION-COCO 代表了其中的一个 6 亿子集，由 BLIP 生成 caption。我们通过过滤文本质量、图像长宽比和 clip score 等因素对这些数据集进行了细化，得到了一个 300 万对的高质量语料库。为了提高整体图像生成的保真度，我们使用高质量的 LAION-Aesthetics 子集和 Midjourney 的合成数据集 JourneyDB 来补充我们的数据。</span></p><p>&emsp;&emsp;<span>我们还合并了图像-文本交错数据，以使模型适应交错的模式。我们部署了 Multimodal-C4 (MMC4) 数据集，这是 text-only C4 的增强版。具体地说，我们使用 MMC4-core 划分，由 730 万个文档组成。</span></p><p><strong><span>Speech &amp; Text</span></strong><span> 我们收集了几个大规模的英语自动语音识别 (Automatic Speech Recognition, ASR) 数据集，包括 Gigaspeech、Common Voice 和Multilingual LibriSpeech (MLS)。这些数据集分别来自在线平台、志愿者众包和有声读物，共同构成了一个 57,000 小时的语音文本对语料库，涵盖了各种各样的演讲者、领域和录音环境。</span></p><p><strong><span>Music&amp;Text</span></strong><span> 我们进行一个广泛的数据收集过程，从互联网上抓取了超过一百万的音乐视频。核心步骤包括使用 Spotify API 将这些视频的标题与相应的歌曲进行匹配。随后，我们为每个音乐音频收集了一组全面的元数据，包括视频标题、描述、关键词、播放列表名称和 Spotify 歌词。该元数据被格式化为 JSON 并输入GPT4 进行处理。GPT-4 的作用是关键的智能 caption 生成器；它利用嘈杂的metadata 提取有意义的信息，并将其简洁地总结成连贯的句子。这种方法允许我们为大量的音乐音频生成高质量的文本 caption，有效地减少了数据集中幻觉的发生。</span></p><p><strong><span>Training Sample Construction.</span></strong><span> 为了训练语言模型 (LM) ，我们使用各种模板来构建多模态句子，然后 LM 对其进行自回归处理。</span><em><span>更多训练细节见 Appendix A.2。</span></em><span>此外，我们观察到句子长度在不同模态和数据集上的显著差异。为了提高训练效率，来自同一数据集的样本被连接成一个长序列，支持模型的最大序列长度。因此，序列中的每个 token 都会造成损失。</span></p><blockquote><p><strong><font face="Times New Roman"><span>A.2 pre-training</span></font></strong></p><p><span>我们使用各种模板来构建多模态句子，确保我们的预训练数据具有多样化的频谱。每个非文本模态的内容都通过放置在开头和结尾的特殊 tokens 来标识。通常，配对数据包括非文本形态 (X) — 例如图像、语音或音乐 — 及其相应的文本，这些文本可以是 caption 或 transcription </span><em><span>(音标?)</span></em><span>。我们提示 OpenAI GPT-4 生成数百个</span><strong><span>双向指令</span></strong><span> </span><font color=RoyalBlue><span>(下图两个指令模板分别对应双向指令)</span></font><span>，具体来说是 X-to-text 或 text-to-X，例如 “Please generate an image based on the provided text.”。给定 token 序列 (S) 和相关文本 (T)，我们从预先建立的池中随机选择与指令 (I) 一起的生成方向，形成一个三元组 (I, S, T)。然后使用模板将该三元组合并到序列中。</span></p><div class="columns is-centered" id="prompt_template">
<center><img src="prompt_template.png" width="90%"></center>
</div><p><span>对于交错的多模态数据，比如图像和文本穿插的 web 文档，我们直接用相应的 tokens 序列代替非文本内容，因为它们自然地形成了句子。</span></p><p><span>由于大多数图像和音乐数据来自网络，因此存在一定程度的噪声，会影响多模态生成的质量。因此，在最初的预训练之后，我们有选择性地使用高质量的数据集——JourneyDB 和 LAION-Aesthetics 进行 text-to-image 生成，并使用 LAION-COCO 进行 image captioning。对于音乐数据，我们采用了AnyInstruct-108k 数据集。剩余的数据保持不变，我们继续对模型进行额外的4000步预训练。</span></p><p><span>我们在表 8 中报告了 AnyGPT 的详细训练超参数。</span></p><div class="columns is-centered" id="table_8">
<center><img src="table_8.png" width="90%"></center>
</div></blockquote><h2 id='42-multimodal-interleaved-instruction-data-construction-data'><font face="Times New Roman"><span>4.2 Multimodal Interleaved Instruction Data Construction Data</span></font></h2><p><span>有效的人机交互应该允许以各种交错的方式交换信息。然而，对话中越来越多的模式大大复杂化了数据收集过程。据我们所知，目前还没有涉及两种以上模态的大规模指令数据集。这对发展一个能够管理多种相互交织的模态的对话的综合模型造成了重大限制。</span></p><p><span>为了克服这一限制，我们从最新的数据合成研究中汲取灵感，并使用生成模型构建由 108k 个多轮会话样本组成的数据集。通过精心策划，每个合成对话以交错的方式集成了多种形态——文本、语音、图像和音乐。具体来说，我们的数据合成过程分两个阶段进行，如图3所示。</span></p><div class="columns is-centered" id="figure_3">
<center><img src="figure_3.png" width="90%"></center>
</div><p><em><span>Figure 3: 多模态交错指令数据集 AnyInstruct 的构建过程分为两个阶段：包含多模态元素的基于文本的对话生成和文本到多模态的转换。第一阶段生成主题、场景和文本对话，而第二阶段生成最终的多模态对话。</span></em></p><p><strong><span>Generation of text-based conversations incorporating multimodal elements.</span></strong><span> 在这个阶段，我们使用 GPT-4 来生成一系列基于文本的对话。值得注意的是，我们在这些对话中以文本描述的形式合并了非文本模态。为了确保大规模的高质量数据，我们将此阶段分为三个步骤。(1) 最初，我们头脑风暴 100 个元主题，以涵盖与视听元素相关的广泛场景，并使用 GPT-4 将这些元主题扩展到 20,000 个具体的主题。(2) 随后，我们提示 LLM 根据这些主题生成具体的对话场景。考虑到基于文本的 LLM 在生成多模态元素方面的内在约束，我们准备了几个演示，包括尽可能多的模态组合。在生成场景时，从这个演示池中抽取一个子集，作为 LLM 的示例。这种方法指导模型有效地综合各种上下文适当的会话场景。(3) 最后，利用 GPT-4 生成基于场景的多轮对话。在这些综合对话中，包括图像和音乐在内的多模态元素通过详细的文本表示来描述。我们策划了各种各样的对话示例，类似于场景生成，以促使模型以尽可能多的方式创建对话。因此，我们以纯文本格式编译了大量的多模态会话数据。</span></p><p><strong><span>Text-to-Multimodality Conversion.</span></strong><span> 在这个阶段，我们使用先进的生成模型将文本描述转换为多模态元素。我们使用 OpenAI 的 DALL-E-3 进行图像生成，MusicGen 进行音乐作曲，Microsoft Azure 的 text-to-speech API (Microsoft) 用于从用户指令和模型的文本响应中进行语音合成。</span></p><p>&emsp;&emsp;<span>经过过滤，我们得到了 108k 的高质量多模态对话数据集，具有多种多模态组合。该数据集包括大约 205k 图像，503k 录音和 113k 音乐曲目。此外，我们通过从现有的适合口语叙述的纯文本指令数据集中提取对话来增强我们的数据集。通过使用 text-to-speech 模型，生成了10万个语音对话。</span></p><p>&emsp;&emsp;<span>两阶段方法有效地大规模收集了各种高质量的多模态对话。</span><em><span>Appendix D</span></em><span> 提供了数据合成过程中使用的提示。</span></p><h1 id='5-experiment'><font face="Times New Roman"><span>5 Experiment</span></font></h1><h2 id='51-evaluation'><font face="Times New Roman"><span>5.1 Evaluation</span></font></h2><p><span>我们评估了预训练的基础 AnyGPT (Section 3) 的基本能力，涵盖了所有模态的多模态理解和生成任务。本评估旨在测试在预训练过程中不同模态之间的一致性。具体来说，我们测试了每种模态的 text-to-X 和 X-to-text 任务，其中 X 分别是图像、音乐和语音。</span></p><p><span>为了模拟真实的场景，所有的评估都是在 zero-shot 模式下进行的。这意味着在评估过程中，AnyGPT 不会对下游训练样本进行微调或预训练。这种具有挑战性的评估设置要求模型泛化到未知的测试分布，展示 AnyGPT 在不同模态下的通才能力。评价结果表明，AnyGPT 作为一个通用型多模态语言模型，在各种多模态理解和生成任务上取得了良好的表现。</span></p><h3 id='511-image'><font face="Times New Roman"><span>5.1.1 Image</span></font></h3><div class="columns is-centered" id="table_2">
<center><img src="table_2.png" width="40%"></center>
</div><p><strong><span>Image Understanding</span></strong><span> 我们评估 AnyGPT 在 image captioning 任务上的图像理解能力。对比结果见 Table 2。我们使用 MS-COCO 2014 captioning benchmark，并采用 </span><em><span>Karpathy split testset</span></em><span>  遵循先前的研究 (</span><a href='http://arxiv.org/abs/2301.12597'><span>BLIP-2</span></a><span>; </span><a href='https://arxiv.org/abs/2305.11846'><span>CoDi</span></a><span>)。</span></p><div class="columns is-centered" id="table_3">
<center><img src="table_3.png" width="40%"></center>
</div><p><strong><span>Image Generation</span></strong><span> text-to-image 生成任务的结果如 Table 3 所示。为了确保与以往研究的一致性(</span><a href='http://arxiv.org/abs/2305.17216'><span>GILL</span></a><span>; </span><a href='https://arxiv.org/abs/2310.01218'><span>SEED-LLaMA</span></a><span>; </span><a href='http://arxiv.org/abs/2312.13286'><span>Emu 2</span></a><span>)，我们从 MS-COCO 验证集中随机选择 30k 张图像，并使用 </span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="9.279ex" height="1.952ex" role="img" focusable="false" viewBox="0 -705 4101.2 862.8" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.357ex;"><defs><path id="MJX-14-TEX-N-43" d="M56 342Q56 428 89 500T174 615T283 681T391 705Q394 705 400 705T408 704Q499 704 569 636L582 624L612 663Q639 700 643 704Q644 704 647 704T653 705H657Q660 705 666 699V419L660 413H626Q620 419 619 430Q610 512 571 572T476 651Q457 658 426 658Q322 658 252 588Q173 509 173 342Q173 221 211 151Q232 111 263 84T328 45T384 29T428 24Q517 24 571 93T626 244Q626 251 632 257H660L666 251V236Q661 133 590 56T403 -21Q262 -21 159 83T56 342Z"></path><path id="MJX-14-TEX-N-4C" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q48 680 182 680Q324 680 348 683H360V637H333Q273 637 258 635T233 622L232 342V129Q232 57 237 52Q243 47 313 47Q384 47 410 53Q470 70 498 110T536 221Q536 226 537 238T540 261T542 272T562 273H582V268Q580 265 568 137T554 5V0H25V46H58Q100 47 109 49T128 61V622Z"></path><path id="MJX-14-TEX-N-49" d="M328 0Q307 3 180 3T32 0H21V46H43Q92 46 106 49T126 60Q128 63 128 342Q128 620 126 623Q122 628 118 630T96 635T43 637H21V683H32Q53 680 180 680T328 683H339V637H317Q268 637 254 634T234 623Q232 620 232 342Q232 63 234 60Q238 55 242 53T264 48T317 46H339V0H328Z"></path><path id="MJX-14-TEX-N-50" d="M130 622Q123 629 119 631T103 634T60 637H27V683H214Q237 683 276 683T331 684Q419 684 471 671T567 616Q624 563 624 489Q624 421 573 372T451 307Q429 302 328 301H234V181Q234 62 237 58Q245 47 304 46H337V0H326Q305 3 182 3Q47 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM507 488Q507 514 506 528T500 564T483 597T450 620T397 635Q385 637 307 637H286Q237 637 234 628Q231 624 231 483V342H302H339Q390 342 423 349T481 382Q507 411 507 488Z"></path><path id="MJX-14-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-14-TEX-I-1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path><path id="MJX-14-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path id="MJX-14-TEX-I-1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-14-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="43" xlink:href="#MJX-14-TEX-N-43"></use></g><g data-mml-node="mi" transform="translate(722,0)"><use data-c="4C" xlink:href="#MJX-14-TEX-N-4C"></use></g><g data-mml-node="mi" transform="translate(1347,0)"><use data-c="49" xlink:href="#MJX-14-TEX-N-49"></use></g><g data-mml-node="mi" transform="translate(1708,0)"><use data-c="50" xlink:href="#MJX-14-TEX-N-50"></use></g></g></g><g data-mml-node="TeXAtom" transform="translate(2422,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D460" xlink:href="#MJX-14-TEX-I-1D460"></use></g><g data-mml-node="mi" transform="translate(469,0)"><use data-c="1D450" xlink:href="#MJX-14-TEX-I-1D450"></use></g><g data-mml-node="mi" transform="translate(902,0)"><use data-c="1D45C" xlink:href="#MJX-14-TEX-I-1D45C"></use></g><g data-mml-node="mi" transform="translate(1387,0)"><use data-c="1D45F" xlink:href="#MJX-14-TEX-I-1D45F"></use></g><g data-mml-node="mi" transform="translate(1838,0)"><use data-c="1D452" xlink:href="#MJX-14-TEX-I-1D452"></use></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">C</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">I</mi><mi mathvariant="normal">P</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi></mrow></msub></math></mjx-assistive-mml></mjx-container><script type="math/tex">{\rm{CLIP}}_{score}</script><span>​ 作为评估标准。该指标基于 CLIP-ViT-L 计算生成的图像与其对应的真实图像的 caption 之间的相似性得分。</span></p><h3 id='512-speech'><font face="Times New Roman"><span>5.1.2 Speech</span></font></h3><div class="columns is-centered" id="table_5">
<center><img src="table_5.png" width="40%"></center>
</div><p><strong><span>ASR</span></strong><span> 我们通过计算 LibriSpeech 数据集的 test-clean 子集上的 Word Error Rate (WER) 来评估 AnyGPT 在 Automatic Speech Recognition (ASR) 任务上的性能。我们使用 Wav2vec 2.0 和 Whisper Large V2 作为 baselines。Wav2vec 2.0 经过了 6 万小时的语音预训练，并在 LibriSpeech 上进行了微调，而 Whisper Large V2 在 zero-shot 环境中进行了评估，但经过了 68 万小时的语音训练。结果如表5所示。</span></p><div class="columns is-centered" id="table_4">
<center><img src="table_4.png" width="40%"></center>
</div><p><strong><span>TTS</span></strong><span> 我们对 VCTK 数据集进行了 zero-shot Text-to-Speech (TTS) 评估。结果如 Table 4 所示。我们用说话人相似度和 Word Error Rate (WER) 来评估 TTS 系统，其中 WER 关注的是语音质量。</span><em><span>更多的实验细节可以在 Appendix C 中找到。</span></em></p><blockquote><p><strong><font face="Times New Roman"><span>C Evaluation</span></font></strong></p><div class="columns is-centered" id="table_9">
<center><img src="table_9.png" width="90%"></center>
</div><p><span>我们对 VCTK 数据集进行了zero-shot Text-to-Speech (TTS) 评估。我们的训练数据和 VCTK 数据集之间的说话人没有重叠。我们随机从每个说话者中选择一个 3 秒的片段作为语音提示，并附带一个单独的文本作为输入。</span></p><p><span>结果如 Table 4 所示。我们用说话人相似度和 WER 来评价 TTS 系统。为了评估生成语音和提示语音之间的说话人相似度，我们使用了 WavLM-TDNN2。它可以同时为生成的语音和提示语音生成说话人 embeddings，然后计算这些 embeddings 之间的余弦相似度。使用 Whisper 介质模型转录生成的语音计算 WER, WER 越低表示合成语音的质量越高。</span></p><p><span>我们将我们的模型与 VALL-E 和 USLM 进行比较，两者都使用两个自回归模型进行语音建模。它们分别使用 Encodec 和 SpeechTokenizer 作为语音 tokenizers。</span></p></blockquote><h2 id='513-music'><font face="Times New Roman"><span>5.1.3 Music</span></font></h2><p><span>我们在 MusicCaps benchmark 上评估 AnyGPT 在音乐理解和生成任务上的表现。我们利用 </span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="9.279ex" height="1.952ex" role="img" focusable="false" viewBox="0 -705 4101.2 862.8" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.357ex;"><defs><path id="MJX-14-TEX-N-43" d="M56 342Q56 428 89 500T174 615T283 681T391 705Q394 705 400 705T408 704Q499 704 569 636L582 624L612 663Q639 700 643 704Q644 704 647 704T653 705H657Q660 705 666 699V419L660 413H626Q620 419 619 430Q610 512 571 572T476 651Q457 658 426 658Q322 658 252 588Q173 509 173 342Q173 221 211 151Q232 111 263 84T328 45T384 29T428 24Q517 24 571 93T626 244Q626 251 632 257H660L666 251V236Q661 133 590 56T403 -21Q262 -21 159 83T56 342Z"></path><path id="MJX-14-TEX-N-4C" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q48 680 182 680Q324 680 348 683H360V637H333Q273 637 258 635T233 622L232 342V129Q232 57 237 52Q243 47 313 47Q384 47 410 53Q470 70 498 110T536 221Q536 226 537 238T540 261T542 272T562 273H582V268Q580 265 568 137T554 5V0H25V46H58Q100 47 109 49T128 61V622Z"></path><path id="MJX-14-TEX-N-49" d="M328 0Q307 3 180 3T32 0H21V46H43Q92 46 106 49T126 60Q128 63 128 342Q128 620 126 623Q122 628 118 630T96 635T43 637H21V683H32Q53 680 180 680T328 683H339V637H317Q268 637 254 634T234 623Q232 620 232 342Q232 63 234 60Q238 55 242 53T264 48T317 46H339V0H328Z"></path><path id="MJX-14-TEX-N-50" d="M130 622Q123 629 119 631T103 634T60 637H27V683H214Q237 683 276 683T331 684Q419 684 471 671T567 616Q624 563 624 489Q624 421 573 372T451 307Q429 302 328 301H234V181Q234 62 237 58Q245 47 304 46H337V0H326Q305 3 182 3Q47 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM507 488Q507 514 506 528T500 564T483 597T450 620T397 635Q385 637 307 637H286Q237 637 234 628Q231 624 231 483V342H302H339Q390 342 423 349T481 382Q507 411 507 488Z"></path><path id="MJX-14-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-14-TEX-I-1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path><path id="MJX-14-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path id="MJX-14-TEX-I-1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-14-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="43" xlink:href="#MJX-14-TEX-N-43"></use></g><g data-mml-node="mi" transform="translate(722,0)"><use data-c="4C" xlink:href="#MJX-14-TEX-N-4C"></use></g><g data-mml-node="mi" transform="translate(1347,0)"><use data-c="49" xlink:href="#MJX-14-TEX-N-49"></use></g><g data-mml-node="mi" transform="translate(1708,0)"><use data-c="50" xlink:href="#MJX-14-TEX-N-50"></use></g></g></g><g data-mml-node="TeXAtom" transform="translate(2422,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D460" xlink:href="#MJX-14-TEX-I-1D460"></use></g><g data-mml-node="mi" transform="translate(469,0)"><use data-c="1D450" xlink:href="#MJX-14-TEX-I-1D450"></use></g><g data-mml-node="mi" transform="translate(902,0)"><use data-c="1D45C" xlink:href="#MJX-14-TEX-I-1D45C"></use></g><g data-mml-node="mi" transform="translate(1387,0)"><use data-c="1D45F" xlink:href="#MJX-14-TEX-I-1D45F"></use></g><g data-mml-node="mi" transform="translate(1838,0)"><use data-c="1D452" xlink:href="#MJX-14-TEX-I-1D452"></use></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">C</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">I</mi><mi mathvariant="normal">P</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi></mrow></msub></math></mjx-assistive-mml></mjx-container><script type="math/tex">{\rm{CLIP}}_{score}</script><span> 分数作为客观指标，衡量生成的音乐与文本描述之间的相似性。</span></p><p><span>对于 music captioning 的评价，我们发现现有的客观指标在 music captioning 任务中的表现时可能受到限制。音乐的多样性和主观性导致了个人对音乐的不同看法。只有特定的音乐流派和乐器才具有易于识别的鲜明特征。虽然最近的研究 (</span><a href='https://arxiv.org/abs/2310.07160'><span>LLark</span></a><span>) 已经探讨了这个问题，但它仍然是一个具有挑战性的问题。为了保证客观的评价，我们计算了&lt;music，real caption&gt;对和&lt;music，generated caption&gt;对的 </span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="9.279ex" height="1.952ex" role="img" focusable="false" viewBox="0 -705 4101.2 862.8" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.357ex;"><defs><path id="MJX-14-TEX-N-43" d="M56 342Q56 428 89 500T174 615T283 681T391 705Q394 705 400 705T408 704Q499 704 569 636L582 624L612 663Q639 700 643 704Q644 704 647 704T653 705H657Q660 705 666 699V419L660 413H626Q620 419 619 430Q610 512 571 572T476 651Q457 658 426 658Q322 658 252 588Q173 509 173 342Q173 221 211 151Q232 111 263 84T328 45T384 29T428 24Q517 24 571 93T626 244Q626 251 632 257H660L666 251V236Q661 133 590 56T403 -21Q262 -21 159 83T56 342Z"></path><path id="MJX-14-TEX-N-4C" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q48 680 182 680Q324 680 348 683H360V637H333Q273 637 258 635T233 622L232 342V129Q232 57 237 52Q243 47 313 47Q384 47 410 53Q470 70 498 110T536 221Q536 226 537 238T540 261T542 272T562 273H582V268Q580 265 568 137T554 5V0H25V46H58Q100 47 109 49T128 61V622Z"></path><path id="MJX-14-TEX-N-49" d="M328 0Q307 3 180 3T32 0H21V46H43Q92 46 106 49T126 60Q128 63 128 342Q128 620 126 623Q122 628 118 630T96 635T43 637H21V683H32Q53 680 180 680T328 683H339V637H317Q268 637 254 634T234 623Q232 620 232 342Q232 63 234 60Q238 55 242 53T264 48T317 46H339V0H328Z"></path><path id="MJX-14-TEX-N-50" d="M130 622Q123 629 119 631T103 634T60 637H27V683H214Q237 683 276 683T331 684Q419 684 471 671T567 616Q624 563 624 489Q624 421 573 372T451 307Q429 302 328 301H234V181Q234 62 237 58Q245 47 304 46H337V0H326Q305 3 182 3Q47 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM507 488Q507 514 506 528T500 564T483 597T450 620T397 635Q385 637 307 637H286Q237 637 234 628Q231 624 231 483V342H302H339Q390 342 423 349T481 382Q507 411 507 488Z"></path><path id="MJX-14-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-14-TEX-I-1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path><path id="MJX-14-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path id="MJX-14-TEX-I-1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-14-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="43" xlink:href="#MJX-14-TEX-N-43"></use></g><g data-mml-node="mi" transform="translate(722,0)"><use data-c="4C" xlink:href="#MJX-14-TEX-N-4C"></use></g><g data-mml-node="mi" transform="translate(1347,0)"><use data-c="49" xlink:href="#MJX-14-TEX-N-49"></use></g><g data-mml-node="mi" transform="translate(1708,0)"><use data-c="50" xlink:href="#MJX-14-TEX-N-50"></use></g></g></g><g data-mml-node="TeXAtom" transform="translate(2422,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D460" xlink:href="#MJX-14-TEX-I-1D460"></use></g><g data-mml-node="mi" transform="translate(469,0)"><use data-c="1D450" xlink:href="#MJX-14-TEX-I-1D450"></use></g><g data-mml-node="mi" transform="translate(902,0)"><use data-c="1D45C" xlink:href="#MJX-14-TEX-I-1D45C"></use></g><g data-mml-node="mi" transform="translate(1387,0)"><use data-c="1D45F" xlink:href="#MJX-14-TEX-I-1D45F"></use></g><g data-mml-node="mi" transform="translate(1838,0)"><use data-c="1D452" xlink:href="#MJX-14-TEX-I-1D452"></use></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">C</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">I</mi><mi mathvariant="normal">P</mi></mrow></mrow><mrow data-mjx-texclass="ORD"><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi></mrow></msub></math></mjx-assistive-mml></mjx-container><script type="math/tex">{\rm{CLIP}}_{score}</script><span>​ 进行比较。这些分数是整个测试集的平均值。</span></p><div class="columns is-centered" id="table_6">
<center><img src="table_6.png" width="40%"></center>
</div><p>&nbsp;</p><h2 id='52-example-demonstrations'><font face="Times New Roman"><span>5.2 Example Demonstrations</span></font></h2><p><span>在 AnyInstruct-108k 数据集上进行微调后，AnyGPT 展示了在 any-to-any 多模态对话中的能力和潜力。</span><em><span>我们在 Appendix E 中提供了令人信服的 AnyGPT 会话示例。</span></em><span>这些示例展示了 AnyGPT 能够在任何组合中理解和推理各种模态的内容。具体来说，AnyGPT 可以理解由文本、语音、图像、音乐等多种模态交织而成的指令，并能熟练地选择合适的多模态组合进行回复。语义-声学分层建模的两阶段框架使 AnyGPT 能够生成与 3 秒语音提示的音色和情感相匹配的语音响应。要获取更多示例并体验语音和音乐内容，</span><em><span>我们强烈建议访问 </span><a href='https://junzhan2000.github.io/AnyGPT.github.io/'><span>demo page</span></a><span>。</span></em></p><h1 id='6-conclusion'><font face="Times New Roman"><span>6 Conclusion</span></font></h1><p><span>在这项工作中，我们介绍了 AnyGPT，这是一种 any-to-any 的多模态语言模型，它利用离散表示来统一处理各种模态，包括语音、文本、图像和音乐。离散的多模态表示促进了新模态的无缝集成，可与合并外语相媲美，而无需改变现有的 LLM 架构或训练范例。为了使模型能够处理多模态输入和输出的任意组合，我们合成了第一个大规模任意对任意多模态指令数据集 AnyInstruct-108k，该数据集由复杂交织各种模态的多轮对话组成。实验结果表明，AnyGPT在各种跨模态任务中取得了良好的效果，表明离散表示可以在统一的大语言模型中有效方便地统一多个模态。</span></p></div></div>
</body>
</html>