# Awesome-Multimodal-Papers

A curated list of awesome Multimodal studies.


- [Awesome-Multimodal-Papers](#awesome-multimodal-papers)
  - [Multimodal Papers](#multimodal-papers)
    - [Large Multimodal Model](#large-multimodal-model)
    - [LMM Benchmark](#lmm-benchmark)
    - [Multimodal Dialogue](#multimodal-dialogue)
    - [Multimodal Learning](#multimodal-learning)
    - [Text/Image-to-Image Generation](#textimage-to-image-generation)
    - [Multimodal Dataset](#multimodal-dataset)
    - [Multimodal Summary](#multimodal-summary)

## Multimodal Papers

### Large Multimodal Model

|  Title  |   Venue  |   Date   |   Code   |Supplement|
|:--------|:--------:|:--------:|:--------:|:--------:|
| [MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training](https://arxiv.org/abs/2403.09611) | arXiv | 2024-03-14 | - | - |
| [UniCode: Learning a Unified Codebook for Multimodal Large Language Models](https://arxiv.org/abs/2403.09072) | arXiv | 2024-03-14 | - | - |
| [RegionGPT: Towards Region Understanding Vision Language Model](https://arxiv.org/abs/2403.02330) | CVPR 2024 | 2024-03-04 | - | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://guoqiushan.github.io/regiongpt.github.io/) |
| [All in an Aggregated Image for In-Image Learning](https://arxiv.org/abs/2402.17971) | arXiv | 2024-02-28 | [![Star](https://img.shields.io/github/stars/AGI-Edgerunners/IIL.svg?style=social&label=Star)](https://github.com/AGI-Edgerunners/IIL) | - |
| [Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners](https://arxiv.org/abs/2402.17723) | CVPR 2024 | 2024-02-27 | [![Star](https://img.shields.io/github/stars/yzxing87/Seeing-and-Hearing.svg?style=social&label=Star)](https://github.com/yzxing87/Seeing-and-Hearing) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://yzxing87.github.io/Seeing-and-Hearing/) |
| [TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages](https://arxiv.org/abs/2402.16021) | arXiv | 2024-02-25 | - | - |
| [LLMBind: A Unified Modality-Task Integration Framework](https://arxiv.org/abs/2402.14891) | arXiv | 2024-02-22 | - | - |
| [AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling](https://arxiv.org/abs/2402.12226) | arXiv | 2024-02-19 | [![Star](https://img.shields.io/github/stars/OpenMOSS/AnyGPT.svg?style=social&label=Star)](https://github.com/OpenMOSS/AnyGPT) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://junzhan2000.github.io/AnyGPT.github.io/) |
| [MobileVLM V2: Faster and Stronger Baseline for Vision Language Model](https://arxiv.org/abs/2402.03766) | arXiv | 2024-02-06 | [![Star](https://img.shields.io/github/stars/Meituan-AutoML/MobileVLM.svg?style=social&label=Star)](https://github.com/Meituan-AutoML/MobileVLM) | - |
| [Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization](https://arxiv.org/abs/2402.03161) | arXiv | 2024-02-05 | [![Star](https://img.shields.io/github/stars/jy0205/LaVIT.svg?style=social&label=Star)](https://github.com/jy0205/LaVIT) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://video-lavit.github.io/) |
| [Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action](https://arxiv.org/abs/2312.17172) | arXiv | 2023-12-28 | [![Star](https://img.shields.io/github/stars/allenai/unified-io-2.svg?style=social&label=Star)](https://github.com/allenai/unified-io-2) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://unified-io-2.allenai.org/) |
| [MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices](https://arxiv.org/abs/2312.16886) | arXiv | 2023-12-28 | [![Star](https://img.shields.io/github/stars/Meituan-AutoML/MobileVLM.svg?style=social&label=Star)](https://github.com/Meituan-AutoML/MobileVLM) | - |
| [Generative Multimodal Models are In-Context Learners](https://arxiv.org/abs/2312.13286) (Emu2) | CVPR 2024 | 2023-12-20 | [![Star](https://img.shields.io/github/stars/baaivision/Emu.svg?style=social&label=Star)](https://github.com/baaivision/Emu) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://baaivision.github.io/emu2/) |
| [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805) | arXiv | 2023-12-19 | - | [![Project Page](https://img.shields.io/badge/Google-Gemini-blue.svg)](https://gemini.google.com/) |
| [Osprey: Pixel Understanding with Visual Instruction Tuning](https://arxiv.org/abs/2312.10032) | CVPR 2024 | 2023-12-15 | [![Star](https://img.shields.io/github/stars/CircleRadon/Osprey.svg?style=social&label=Star)](https://github.com/CircleRadon/Osprey) | - |
| [VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation](https://arxiv.org/abs/2312.09251) | arXiv | 2023-12-14 | [![Star](https://img.shields.io/github/stars/AILab-CVC/VL-GPT.svg?style=social&label=Star)](https://github.com/AILab-CVC/VL-GPT) | - |
| [Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models](https://arxiv.org/abs/2312.06109) | arXiv | 2023-12-11 | [![Star](https://img.shields.io/github/stars/Ucas-HaoranWei/Vary.svg?style=social&label=Star)](https://github.com/Ucas-HaoranWei/Vary) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://varybase.github.io/) |
| [Prompt Highlighter: Interactive Control for Multi-Modal LLMs](https://arxiv.org/abs/2312.04302) | CVPR 2024 | 2023-12-07 | [![Star](https://img.shields.io/github/stars/dvlab-research/Prompt-Highlighter.svg?style=social&label=Star)](https://github.com/dvlab-research/Prompt-Highlighter) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://julianjuaner.github.io/projects/PromptHighlighter/) |
| [PixelLM: Pixel Reasoning with Large Multimodal Model](https://arxiv.org/abs/2312.02228) | CVPR 2024 | 2023-12-04 | [![Star](https://img.shields.io/github/stars/MaverickRen/PixelLM.svg?style=social&label=Star)](https://github.com/MaverickRen/PixelLM) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://pixellm.github.io/) |
| [APoLLo : Unified Adapter and Prompt Learning for Vision Language Models](https://aclanthology.org/2023.emnlp-main.629/) | EMNLP 2023 | 2023-12-04 | [![Star](https://img.shields.io/github/stars/schowdhury671/APoLLo.svg?style=social&label=Star)](https://github.com/schowdhury671/APoLLo) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://gamma.umd.edu/pro/vision_language/apollo/) |
| [CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation](https://arxiv.org/abs/2311.18775) | arXiv | 2023-11-30 | [![Star](https://img.shields.io/github/stars/microsoft/i-Code.svg?style=social&label=Star)](https://github.com/microsoft/i-Code/tree/main/CoDi-2) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://codi-2.github.io/) |
| [LLMGA: Multimodal Large Language Model based Generation Assistant](https://arxiv.org/abs/2311.16500) | arXiv | 2023-11-27 | [![Star](https://img.shields.io/github/stars/dvlab-research/LLMGA.svg?style=social&label=Star)](https://github.com/dvlab-research/LLMGA) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://llmga.github.io/) |
| [PG-Video-LLaVA: Pixel Grounding Large Video-Language Models](https://arxiv.org/abs/2311.13435) | arXiv | 2023-11-22 | [![Star](https://img.shields.io/github/stars/mbzuai-oryx/Video-LLaVA.svg?style=social&label=Star)](https://github.com/mbzuai-oryx/Video-LLaVA) | - |
| [ShareGPT4V: Improving Large Multi-Modal Models with Better Captions](https://arxiv.org/abs/2311.12793) | arXiv | 2023-11-21 | [![Star](https://img.shields.io/github/stars/InternLM/InternLM-XComposer.svg?style=social&label=Star)](https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://sharegpt4v.github.io/) |
| [LION : Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge](https://arxiv.org/abs/2311.11860) | CVPR 2024 | 2023-11-20 | [![Star](https://img.shields.io/github/stars/rshaojimmy/JiuTian.svg?style=social&label=Star)](https://github.com/rshaojimmy/JiuTian) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://rshaojimmy.github.io/Projects/JiuTian-LION) |
| [Video-LLaVA: Learning United Visual Representation by Alignment Before Projection](https://arxiv.org/abs/2311.10122) | arXiv | 2023-11-16 | [![Star](https://img.shields.io/github/stars/PKU-YuanGroup/Video-LLaVA.svg?style=social&label=Star)](https://github.com/PKU-YuanGroup/Video-LLaVA) | - |
| [mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration](https://arxiv.org/abs/2311.04257) | arXiv | 2023-11-07 | [![Star](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl.svg?style=social&label=Star)](https://github.com/X-PLUG/mPLUG-Owl) | - |
| [MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning](https://arxiv.org/abs/2310.09478) | arXiv | 2023-10-14 | [![Star](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4.svg?style=social&label=Star)](https://github.com/Vision-CAIR/MiniGPT-4) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://minigpt-v2.github.io/) |
| [EasyGen: Easing Multimodal Generation with a Bidirectional Conditional Diffusion Model and LLMs](https://arxiv.org/abs/2310.08949) | arXiv | 2023-10-13 | [![Star](https://img.shields.io/github/stars/zxy556677/EasyGen.svg?style=social&label=Star)](https://github.com/zxy556677/EasyGen) | - |
| [Mini-DALLE3: Interactive Text to Image by Prompting Large Language Models](https://arxiv.org/abs/2310.07653) | arXiv | 2023-10-11 | [![Star](https://img.shields.io/github/stars/Zeqiang-Lai/Mini-DALLE3.svg?style=social&label=Star)](https://github.com/Zeqiang-Lai/Mini-DALLE3) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://minidalle3.github.io/) |
| [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/abs/2310.03744) (LLaVA-1.5) | arXiv | 2023-10-05 | [![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star)](https://github.com/haotian-liu/LLaVA) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://llava-vl.github.io/) |
| [Kosmos-G: Generating Images in Context with Multimodal Large Language Models](https://arxiv.org/abs/2310.02992) | arXiv | 2023-10-04 | [![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)](https://github.com/microsoft/unilm/tree/master/kosmos-g) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://xichenpan.com/kosmosg/) |
| [MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens](https://arxiv.org/abs/2310.02239) | arXiv | 2023-10-03 | [![Star](https://img.shields.io/github/stars/eric-ai-lab/MiniGPT-5.svg?style=social&label=Star)](https://github.com/eric-ai-lab/MiniGPT-5) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://eric-ai-lab.github.io/minigpt-5.github.io/) |
| [DreamLLM: Synergistic Multimodal Comprehension and Creation](https://arxiv.org/abs/2309.11499) | ICLR 2024 | 2023-09-20 | [![Star](https://img.shields.io/github/stars/RunpeiDong/DreamLLM.svg?style=social&label=Star)](https://github.com/RunpeiDong/DreamLLM) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://dreamllm.github.io/) |
| [MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning](https://arxiv.org/abs/2309.07915) | ICLR 2024 | 2023-09-14 | [![Star](https://img.shields.io/github/stars/HaozheZhao/MIC.svg?style=social&label=Star)](https://github.com/HaozheZhao/MIC) | - |
| [NExT-GPT: Any-to-Any Multimodal LLM](https://arxiv.org/abs/2309.05519) | arXiv | 2023-09-11 | [![Star](https://img.shields.io/github/stars/NExT-GPT/NExT-GPT.svg?style=social&label=Star)](https://github.com/NExT-GPT/NExT-GPT) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://next-gpt.github.io/) |
| [Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization](https://arxiv.org/abs/2309.04669) (LaVIT) | ICLR 2024 | 2023-09-09 | [![Star](https://img.shields.io/github/stars/jy0205/LaVIT.svg?style=social&label=Star)](https://github.com/jy0205/LaVIT) | - |
| [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://arxiv.org/abs/2308.12966) | arXiv | 2023-08-24 | [![Star](https://img.shields.io/github/stars/QwenLM/Qwen-VL.svg?style=social&label=Star)](https://github.com/QwenLM/Qwen-VL) | [![Project Page](https://img.shields.io/badge/阿里-通义千问-blue.svg)](https://tongyi.aliyun.com/qianwen/) |
| [Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages](https://arxiv.org/abs/2308.12038) (VisCPM-Chat/Paint) | ICLR 2024 | 2023-08-23 | [![Star](https://img.shields.io/github/stars/OpenBMB/VisCPM.svg?style=social&label=Star)](https://github.com/OpenBMB/VisCPM) | - |
| [Planting a SEED of Vision in Large Language Model](https://arxiv.org/abs/2307.08041) | ICLR 2024 | 2023-07-16 | [![Star](https://img.shields.io/github/stars/AILab-CVC/SEED.svg?style=social&label=Star)](https://github.com/AILab-CVC/SEED) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://ailab-cvc.github.io/seed/) |
| [Generative Pretraining in Multimodality](https://arxiv.org/abs/2307.05222) (Emu1) | ICLR 2024 | 2023-07-11 | [![Star](https://img.shields.io/github/stars/baaivision/Emu.svg?style=social&label=Star)](https://github.com/baaivision/Emu) | - |
| [SVIT: Scaling up Visual Instruction Tuning](https://arxiv.org/abs/2307.04087) | arXiv | 2023-07-09 | [![Star](https://img.shields.io/github/stars/BAAI-DCAI/Visual-Instruction-Tuning.svg?style=social&label=Star)](https://github.com/BAAI-DCAI/Visual-Instruction-Tuning) | [![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/BAAI/SVIT) |
| [M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning](https://arxiv.org/abs/2306.04387) | arXiv | 2023-06-07 | - | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://m3-it.github.io/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/MMInstruction/M3IT) |
| [Generating Images with Multimodal Language Models](https://arxiv.org/abs/2305.17216) (GILL) | NeurIPS 2023 | 2023-05-26 | [![Star](https://img.shields.io/github/stars/kohjingyu/gill.svg?style=social&label=Star)](https://github.com/kohjingyu/gill) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://jykoh.com/gill) |
| [Any-to-Any Generation via Composable Diffusion](https://proceedings.neurips.cc/paper_files/paper/2023/hash/33edf072fe44f19079d66713a1831550-Abstract-Conference.html) (CoDi-1) | NeurIPS 2023 | 2023-05-19 | [![Star](https://img.shields.io/github/stars/microsoft/i-Code.svg?style=social&label=Star)](https://github.com/microsoft/i-Code/tree/main/i-Code-V3) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://codi-gen.github.io/) |
| [SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities](https://aclanthology.org/2023.findings-emnlp.1055/) | EMNLP 2023 (Findings) | 2023-05-18 | [![Star](https://img.shields.io/github/stars/0nutation/SpeechGPT.svg?style=social&label=Star)](https://github.com/0nutation/SpeechGPT) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://0nutation.github.io/SpeechGPT.github.io/) |
| [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://proceedings.neurips.cc/paper_files/paper/2023/hash/9a6a435e75419a836fe47ab6793623e6-Abstract-Conference.html) | NeurIPS 2023 | 2023-05-11 | [![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star)](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip) | - |
| [MultiModal-GPT: A Vision and Language Model for Dialogue with Humans](https://arxiv.org/abs/2305.04790) | arXiv | 2023-05-08 | [![Star](https://img.shields.io/github/stars/open-mmlab/Multimodal-GPT.svg?style=social&label=Star)](https://github.com/open-mmlab/Multimodal-GPT) | - |
| [VPGTrans: Transfer Visual Prompt Generator across LLMs](https://arxiv.org/abs/2305.01278) | NeurIPS 2023 | 2023-05-02 | [![Star](https://img.shields.io/github/stars/VPGTrans/VPGTrans.svg?style=social&label=Star)](https://github.com/VPGTrans/VPGTrans) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://vpgtrans.github.io/) |
| [mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality](https://arxiv.org/abs/2304.14178) | arXiv | 2023-04-27 | [![Star](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl.svg?style=social&label=Star)](https://github.com/X-PLUG/mPLUG-Owl) | - |
| [MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models](https://arxiv.org/abs/2304.10592) | ICLR 2024 | 2023-04-20 | [![Star](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4.svg?style=social&label=Star)](https://github.com/Vision-CAIR/MiniGPT-4) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://minigpt-4.github.io/) |
| [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) (LLaVA) | NeurIPS 2023 | 2023-04-17 | [![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star)](https://github.com/haotian-liu/LLaVA) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://llava-vl.github.io/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K) |
| [Language Is Not All You Need: Aligning Perception with Language Models](https://proceedings.neurips.cc/paper_files/paper/2023/hash/e425b75bac5742a008d643826428787c-Abstract-Conference.html) | NeurIPS 2023 | 2023-02-27 | [![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)](https://github.com/microsoft/unilm) | - |
| [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923) | arXiv | 2023-02-02 | [![Star](https://img.shields.io/github/stars/amazon-science/mm-cot.svg?style=social&label=Star)](https://github.com/amazon-science/mm-cot) | - |
| [Grounding Language Models to Images for Multimodal Inputs and Outputs](https://arxiv.org/abs/2301.13823) (FROMAGe) | ICML 2023 | 2023-01-31 | [![Star](https://img.shields.io/github/stars/kohjingyu/fromage.svg?style=social&label=Star)](https://github.com/kohjingyu/fromage) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://jykoh.com/fromage) |
| [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) | ICML 2023 | 2023-01-30 | [![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star)](https://github.com/salesforce/LAVIS/tree/main/projects/blip2) | - |
| [Flamingo: a Visual Language Model for Few-Shot Learning](https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html) | NeurIPS 2022 | 2022-04-29 | [![Star](https://img.shields.io/github/stars/mlfoundations/open_flamingo.svg?style=social&label=Star)](https://github.com/mlfoundations/open_flamingo) | -|

### LMM Benchmark

|  Title  |   Venue  |   Date   |   Code   |Supplement|
|:--------|:--------:|:--------:|:--------:|:--------:|
| [SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension](https://arxiv.org/abs/2307.16125) | CVPR 2024 | 2023-07-30 | [![Star](https://img.shields.io/github/stars/AILab-CVC/SEED-Bench.svg?style=social&label=Star)](https://github.com/AILab-CVC/SEED-Bench) | - |

### Multimodal Dialogue

|  Title  |   Venue  |   Date   |   Code   |Supplement|
|:--------|:--------:|:--------:|:--------:|:--------:|
| [DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation](https://arxiv.org/abs/2403.08857) | arXiv | 2024-03-13 | [![Star](https://img.shields.io/github/stars/Centaurusalpha/DialogGen.svg?style=social&label=Star)](https://github.com/Centaurusalpha/DialogGen) | - |
| [STICKERCONV: Generating Multimodal Empathetic Responses from Scratch](https://arxiv.org/abs/2402.01679) | arXiv | 2024-01-20 | [![Star](https://img.shields.io/github/stars/ZhangYiqun018/StickerConv.svg?style=social&label=Star)](https://github.com/ZhangYiqun018/StickerConv) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://neu-datamining.github.io/StickerConv/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/NEUDM/StickerConv) |
| [VDialogUE: A Unified Evaluation Benchmark for Visually-grounded Dialogue](https://arxiv.org/abs/2309.07387) | arXiv | 2023-09-14 | - | - |
| [PaCE: Unified Multi-modal Dialogue Pre-training with Progressive and Compositional Experts](https://aclanthology.org/2023.acl-long.749/) | ACL 2023 | 2023-05-24 | [![Star](https://img.shields.io/github/stars/AlibabaResearch.svg?style=social&label=Star)](https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/pace) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://ruc-aimind.github.io/projects/TikTalk/) |
| [TikTalk: A Multi-Modal Dialogue Dataset for Real-World Chitchat](https://arxiv.org/abs/2301.05880) | ACM MM 2023 | 2023-01-14 | [![Star](https://img.shields.io/github/stars/RUC-AIMind/TikTalk.svg?style=social&label=Star)](https://github.com/RUC-AIMind/TikTalk) | Dataset |
| [MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation](https://aclanthology.org/2023.acl-long.405/) | ACL 2023 | 2022-11-10 | [![Star](https://img.shields.io/github/stars/victorsungo/MMDialog.svg?style=social&label=Star)](https://github.com/victorsungo/MMDialog) | Dataset |
| [Multimodal Dialogue Response Generation](https://aclanthology.org/2022.acl-long.204/) (Divter) | ACL 2022 | 2021-10-16 | - | - |
| [Maria: A Visual Experience Powered Conversational Agent](https://aclanthology.org/2021.acl-long.435/) | ACL 2021 | 2021-05-27 | [![Star](https://img.shields.io/github/stars/jokieleung/Maria.svg?style=social&label=Star)](https://github.com/jokieleung/Maria) | - |
| [Multi-Modal Open-Domain Dialogue](https://aclanthology.org/2021.emnlp-main.398/) | EMNLP 2021 | 2020-10-02 | - | - |
| [Open Domain Dialogue Generation with Latent Images](https://arxiv.org/abs/2004.01981) | AAAI 2021 | 2020-04-04 | - | - |
| [Learning to Respond with Stickers: A Framework of Unifying Multi-Modality in Multi-Turn Dialog](https://arxiv.org/abs/2003.04679) | WWW 2020 | 2020-03-10 | [![Star](https://img.shields.io/github/stars/gsh199449/stickerchat.svg?style=social&label=Star)](https://github.com/gsh199449/stickerchat) | - |

### Multimodal Learning

|  Title  |   Venue  |   Date   |   Code   |Supplement|
|:--------|:--------:|:--------:|:--------:|:--------:|
| [Tokenize Anything via Prompting](https://arxiv.org/abs/2312.09128) | arXiv | 2023-12-14 | [![Star](https://img.shields.io/github/stars/baaivision/tokenize-anything.svg?style=social&label=Star)](https://github.com/baaivision/tokenize-anything) | - |
| [LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment](https://arxiv.org/abs/2310.01852) | ICLR 2024 | 2023-10-03 | [![Star](https://img.shields.io/github/stars/PKU-YuanGroup/LanguageBind.svg?style=social&label=Star)](https://github.com/PKU-YuanGroup/LanguageBind) | - |
| [ImageBind: One Embedding Space To Bind Them All](https://openaccess.thecvf.com/content/CVPR2023/html/Girdhar_ImageBind_One_Embedding_Space_To_Bind_Them_All_CVPR_2023_paper.html) | CVPR 2023 | 2023-05-09 | [![Star](https://img.shields.io/github/stars/facebookresearch/ImageBind.svg?style=social&label=Star)](https://github.com/facebookresearch/ImageBind) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://imagebind.metademolab.com/) |
| [Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks](https://openaccess.thecvf.com/content/CVPR2023/html/Li_Uni-Perceiver_v2_A_Generalist_Model_for_Large-Scale_Vision_and_Vision-Language_CVPR_2023_paper.html) | CVPR 2023 | 2022-11-17 | [![Star](https://img.shields.io/github/stars/fundamentalvision/Uni-Perceiver.svg?style=social&label=Star)](https://github.com/fundamentalvision/Uni-Perceiver) | - |
| [Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks](https://arxiv.org/abs/2208.10442) (BEiT-3) | CVPR 2023 | 2022-08-22 | [![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)](https://github.com/microsoft/unilm/tree/master/beit3) | - |
| [BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers](https://arxiv.org/abs/2208.06366) | arXiv | 2022-08-12 | [![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)](https://github.com/microsoft/unilm/tree/master/beit2) | - |
| [Uni-Perceiver: Pre-Training Unified Architecture for Generic Perception for Zero-Shot and Few-Shot Tasks](https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Uni-Perceiver_Pre-Training_Unified_Architecture_for_Generic_Perception_for_Zero-Shot_and_CVPR_2022_paper.html) | CVPR 2022 | 2021-12-02 | [![Star](https://img.shields.io/github/stars/fundamentalvision/Uni-Perceiver.svg?style=social&label=Star)](https://github.com/fundamentalvision/Uni-Perceiver) |  |
| [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) | ICLR 2022 | 2021-06-15 | [![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)](https://github.com/microsoft/unilm/tree/master/beit) | - |


### Text/Image-to-Image Generation

|  Title  |   Venue  |   Date   |   Code   |Supplement|
|:--------|:--------:|:--------:|:--------:|:--------:|
| [Multi-LoRA Composition for Image Generation](https://arxiv.org/abs/2402.16843) | arXiv | 2024-02-26 | [![Star](https://img.shields.io/github/stars/maszhongming/Multi-LoRA-Composition.svg?style=social&label=Star)](https://github.com/maszhongming/Multi-LoRA-Composition) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://maszhongming.github.io/Multi-LoRA-Composition/) |
| [Brush Your Text: Synthesize Any Scene Text on Images via Diffusion Model](https://arxiv.org/abs/2312.12232) | AAAI 2024 | 2023-12-19 | [![Star](https://img.shields.io/github/stars/ecnuljzhang/brush-your-text.svg?style=social&label=Star)](https://github.com/ecnuljzhang/brush-your-text) | - |
| [SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models](https://arxiv.org/abs/2312.06739) | arXiv | 2023-12-11 | [![Star](https://img.shields.io/github/stars/TencentARC/SmartEdit.svg?style=social&label=Star)](https://github.com/TencentARC/SmartEdit) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://yuzhou914.github.io/SmartEdit/) |
| [InstructAny2Pix: Flexible Visual Editing via Multimodal Instruction Following](https://arxiv.org/abs/2312.06738) | arXiv | 2023-12-11 | [![Star](https://img.shields.io/github/stars/jacklishufan/InstructAny2Pix.svg?style=social&label=Star)](https://github.com/jacklishufan/InstructAny2Pix) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://homepage.jackli.org/projects/instructany2pix.html) |
| [Emu Edit: Precise Image Editing via Recognition and Generation Tasks](https://arxiv.org/abs/2311.10089) | arXiv | 2023-11-16 | - | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://emu-edit.metademolab.com/) |
| [BeautifulPrompt: Towards Automatic Prompt Engineering for Text-to-Image Synthesis](https://aclanthology.org/2023.emnlp-industry.1/) | EMNLP 2023 | 2023-11-12 | [![Star](https://img.shields.io/github/stars/alibaba/EasyNLP.svg?style=social&label=Star)](https://github.com/alibaba/EasyNLP) | [![zhihu](https://img.shields.io/badge/-知乎-000000?logo=zhihu&logoColor=0084FF)](https://zhuanlan.zhihu.com/p/636546340) |
| [AnyText: Multilingual Visual Text Generation And Editing](https://arxiv.org/abs/2311.03054) | ICLR 2024 | 2023-11-06 | [![Star](https://img.shields.io/github/stars/tyxsspa/AnyText.svg?style=social&label=Star)](https://github.com/tyxsspa/AnyText) | - |
| [IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models](https://arxiv.org/abs/2308.06721) | arXiv | 2023-08-13 | [![Star](https://img.shields.io/github/stars/tencent-ailab/IP-Adapter.svg?style=social&label=Star)](https://github.com/tencent-ailab/IP-Adapter) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://ip-adapter.github.io/) |
| [Improving Image Generation with Better Captions](https://cdn.openai.com/papers/dall-e-3.pdf) (DALL-E 3) | OpenAI | 2023 | - | - |
| [Scaling up GANs for Text-to-Image Synthesis](https://arxiv.org/abs/2303.05511) (GigaGAN) | CVPR 2023 | 2023-05-09 | [![Star](https://img.shields.io/github/stars/mingukkang/GigaGAN.svg?style=social&label=Star)](https://github.com/mingukkang/GigaGAN) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://mingukkang.github.io/GigaGAN/) |
| [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543) (ControlNet) | ICCV 2023 | 2023-02-10 | [![Star](https://img.shields.io/github/stars/lllyasviel/ControlNet.svg?style=social&label=Star)](https://github.com/lllyasviel/ControlNet) | - |
| [Scalable Diffusion Models with Transformers](https://arxiv.org/abs/2212.09748) (DiT) | ICCV 2023 | 2022-12-19 | [![Star](https://img.shields.io/github/stars/facebookresearch/DiT.svg?style=social&label=Star)](https://github.com/facebookresearch/DiT) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://www.wpeebles.com/DiT) |
| [InstructPix2Pix: Learning to Follow Image Editing Instructions](https://arxiv.org/abs/2211.09800) | CVPR 2023 | 2022-11-17 | [![Star](https://img.shields.io/github/stars/timothybrooks/instruct-pix2pix.svg?style=social&label=Star)](https://github.com/timothybrooks/instruct-pix2pix) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://www.timothybrooks.com/instruct-pix2pix) |
| [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/abs/2208.12242) | CVPR 2023 | 2022-08-25 | [![Star](https://img.shields.io/github/stars/google/dreambooth.svg?style=social&label=Star)](https://github.com/google/dreambooth) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://dreambooth.github.io/) |
| [Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding](https://arxiv.org/abs/2205.11487) (Imagen) | NeurIPS 2022 | 2022-05-23 | [![Star](https://img.shields.io/github/stars/lucidrains/imagen-pytorch.svg?style=social&label=Star)](https://github.com/lucidrains/imagen-pytorch) | [![Project Page](https://img.shields.io/badge/Google-Imagen-blue.svg)](https://imagen.research.google/) |
| [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125) (DALL-E 2) | OpenAI | 2022-04-13 | [![Star](https://img.shields.io/github/stars/lucidrains/DALLE2-pytorch.svg?style=social&label=Star)](https://github.com/lucidrains/DALLE2-pytorch) | - |
| [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752) (LDM, Stable Diffusion) | CVPR 2022 | 2021-12-20 | [![Star](https://img.shields.io/github/stars/CompVis/latent-diffusion.svg?style=social&label=Star)](https://github.com/CompVis/latent-diffusion) | - |
| [GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models](https://arxiv.org/abs/2112.10741) | ICML 2022 | 2021-12-20 | [![Star](https://img.shields.io/github/stars/openai/glide-text2im.svg?style=social&label=Star)](https://github.com/openai/glide-text2im) | - |
| [NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion](https://arxiv.org/abs/2111.12417) | ECCV 2022 | 2021-11-24 | [![Star](https://img.shields.io/github/stars/microsoft/NUWA.svg?style=social&label=Star)](https://github.com/microsoft/NUWA) | - |
| [SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations](https://arxiv.org/abs/2108.01073) | ICLR 2022 | 2021-08-02 | [![Star](https://img.shields.io/github/stars/ermongroup/SDEdit.svg?style=social&label=Star)](https://github.com/ermongroup/SDEdit) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://sde-image-editing.github.io/) |
| [CogView: Mastering Text-to-Image Generation via Transformers](https://arxiv.org/abs/2105.13290) | NeurIPS 2021 | 2021-05-26 | [![Star](https://img.shields.io/github/stars/THUDM/CogView.svg?style=social&label=Star)](https://github.com/THUDM/CogView) | - |
| [Zero-Shot Text-to-Image Generation](https://arxiv.org/abs/2102.12092) (DALL-E 1) | ICML 2021 | 2021-02-24 | [![Star](https://img.shields.io/github/stars/openai/DALL-E.svg?style=social&label=Star)](https://github.com/openai/DALL-E) | [![Project Page](https://img.shields.io/badge/OpenAI-DALL·E-blue.svg)](https://openai.com/research/dall-e) |
| [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841) (VQ-GAN) | CVPR 2021 | 2020-12-17 | [![Star](https://img.shields.io/github/stars/CompVis/taming-transformers.svg?style=social&label=Star)](https://github.com/CompVis/taming-transformers) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://compvis.github.io/taming-transformers/) |

### Multimodal Dataset

|  Title  |   Venue  |   Date   |Annotation|  Source  |
|:--------|:--------:|:--------:|:--------:|:--------:|
| [STICKERCONV: Generating Multimodal Empathetic Responses from Scratch](https://arxiv.org/abs/2402.01679) | arXiv | 2024-01-20 | Multimodal Empathetic Dialogue | [![Star](https://img.shields.io/github/stars/ZhangYiqun018/StickerConv.svg?style=social&label=Star)](https://github.com/ZhangYiqun018/StickerConv)<br>[![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://neu-datamining.github.io/StickerConv/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/NEUDM/StickerConv) |
| [SVIT: Scaling up Visual Instruction Tuning](https://arxiv.org/abs/2307.04087) | arXiv | 2023-07-09 | Instruction Tuning | [![Star](https://img.shields.io/github/stars/BAAI-DCAI/Visual-Instruction-Tuning.svg?style=social&label=Star)](https://github.com/BAAI-DCAI/Visual-Instruction-Tuning)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/BAAI/SVIT) |
| [M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning](https://arxiv.org/abs/2306.04387) | arXiv | 2023-06-07 | Instruction Tuning | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://m3-it.github.io/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/MMInstruction/M3IT) |
| [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) (LLaVA) | NeurIPS 2023 | 2023-04-17 | Instruction Tuning | [![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star)](https://github.com/haotian-liu/LLaVA)<br>[![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://llava-vl.github.io/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K) |
| [Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text](https://arxiv.org/abs/2304.06939) | NeurIPS D&B 2023 | 2023-04-14 | Interleaved Image-Text | [![Star](https://img.shields.io/github/stars/allenai/mmc4.svg?style=social&label=Star)](https://github.com/allenai/mmc4) |
| [TikTalk: A Multi-Modal Dialogue Dataset for Real-World Chitchat](https://arxiv.org/abs/2301.05880) | ACM MM 2023 | 2023-01-14 | Multimodal Dialogue | [![Star](https://img.shields.io/github/stars/RUC-AIMind/TikTalk.svg?style=social&label=Star)](https://github.com/RUC-AIMind/TikTalk) |
| [MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation](https://aclanthology.org/2023.acl-long.405/) | ACL 2023 | 2022-11-10 | Multimodal Dialogue | [![Star](https://img.shields.io/github/stars/victorsungo/MMDialog.svg?style=social&label=Star)](https://github.com/victorsungo/MMDialog) |
| [LAION-5B: An open large-scale dataset for training next generation image-text models](https://arxiv.org/abs/2210.08402) | NeurIPS 2022 | 2022-10-16 | Image-Text Pairs | [![Project Page](https://img.shields.io/badge/LAION.AI-LAION5B-blue.svg)](https://laion.ai/blog/laion-5b/) |
| [LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs](https://arxiv.org/abs/2111.02114) | NeurIPS Workshop 2021 | 2021-11-03 | Image-Text Pairs | [![Project Page](https://img.shields.io/badge/LAION.AI-LAION400M-blue.svg)](https://laion.ai/blog/laion-400-open-dataset/) |
| [MMConv: An Environment for Multimodal Conversational Search across Multiple Domains](https://dl.acm.org/doi/10.1145/3404835.3462970) | ACM SIGIR 2021 | 2021-07 | Multimodal Dialogue | [![Star](https://img.shields.io/github/stars/liziliao/MMConv.svg?style=social&label=Star)](https://github.com/liziliao/MMConv) |
| [PhotoChat: A Human-Human Dialogue Dataset With Photo Sharing Behavior For Joint Image-Text Modeling](https://aclanthology.org/2021.acl-long.479/) | ACL 2021 | 2021-07-06 | Open-domain Multimodal Dialogue | [![Star](https://img.shields.io/github/stars/google-research/google-research.svg?style=social&label=Star)](https://github.com/google-research/google-research/tree/master/multimodalchat/) |
| [Image-Chat: Engaging Grounded Conversations](https://aclanthology.org/2020.acl-main.219/) | ACL 2020 | 2018-11-02 | Multimodal Dialogue | [![Project Page](https://img.shields.io/badge/ImageChat-blue.svg)](https://parl.ai/projects/image_chat/) |


### Multimodal Summary

|  Title  |   Venue  |   Date   | Latest Update |
|:--------|:--------:|:--------:|:--------:|
| [A Review of Multi-Modal Large Language and Vision Models](https://arxiv.org/abs/2404.01322) | arXiv | 2024-03-28 | - |
| [The (R)Evolution of Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2402.12451) | arXiv | 2024-02-19 | - |
| [MM-LLMs: Recent Advances in MultiModal Large Language Models](https://arxiv.org/abs/2401.13601) | arXiv | 2024-01-24 | 2024-02-20 |
| [Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2311.13165) | 	IEEE BigData 2023 | 2023-11-22 | - |
| [Multimodal Foundation Models: From Specialists to General-Purpose Assistants](https://arxiv.org/abs/2309.10020) | CVPR 2023 | 2023-09-18 | - |
| [Understanding Deep Learning](https://udlbook.github.io/udlbook/) | - | 2023 | - |
| [Large Multimodal Models: Notes on CVPR 2023 Tutorial](https://arxiv.org/abs/2306.14895) | CVPR 2023 | 2023-06-26 | - |
| [A Survey on Multimodal Large Language Models](https://arxiv.org/abs/2306.13549) | arXiv | 2023-06-23 | 2024-04-01 |
| [Multimodal Deep Learning](https://arxiv.org/abs/2301.04856) | arXiv | 2023-01-12 | - |
| [Diffusion Models: A Comprehensive Survey of Methods and Applications](https://arxiv.org/abs/2209.00796) | ACM Computing Surveys | 2022-09-02 | 2024-02-06 |
| [Multimodal Learning with Transformers: A Survey](https://arxiv.org/abs/2206.06488) | IEEE TPAMI 2023 | 2022-01-13<br> |2023-05-10|
| [Multimodal Machine Learning: A Survey and Taxonomy](https://ieeexplore.ieee.org/document/8269806) | IEEE PAMI 2019 | 2017-05-26 | 2017-08-01 |

