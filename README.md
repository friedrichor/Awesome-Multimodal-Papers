# Awesome-Multimodal-Papers

A curated list of awesome Multimodal studies.


- [Awesome-Multimodal-Papers](#awesome-multimodal-papers)
  - [Multimodal Papers](#multimodal-papers)
    - [Large Multimodal Model](#large-multimodal-model)
    - [LMM Benchmark](#lmm-benchmark)
    - [Multimodal Dialogue](#multimodal-dialogue)
    - [Multimodal Learning](#multimodal-learning)
    - [Image-to-Text Generation](#image-to-text-generation)
    - [Text/Image-to-Image Generation](#textimage-to-image-generation)
    - [Video Generation](#video-generation)
    - [Multimodal Dataset](#multimodal-dataset)
    - [Multimodal Summary](#multimodal-summary)

## Multimodal Papers

### Large Multimodal Model

|  Title  |   Venue  |   Date   |   Code   |Supplement|
|:--------|:--------:|:--------:|:--------:|:--------:|
| [Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models](https://arxiv.org/abs/2404.13013) | arXiv | 2024-04-19 | [![Star](https://img.shields.io/github/stars/FoundationVision/Groma.svg?style=social&label=Star)](https://github.com/FoundationVision/Groma) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://groma-mllm.github.io/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/FoundationVision/groma_instruct)|
| [MoVA: Adapting Mixture of Vision Experts to Multimodal Context](https://arxiv.org/abs/2404.13046) | arXiv | 2024-04-19 | [![Star](https://img.shields.io/github/stars/TempleX98/MoVA.svg?style=social&label=Star)](https://github.com/TempleX98/MoVA) | - |
| [Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models](https://arxiv.org/abs/2404.12387) | arXiv | 2024-04-18 | - | [![Project Page](https://img.shields.io/badge/chat-reka.ai-purple.svg)](https://gemini.google.com/)<br>[![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](http://showcase.reka.ai/) |
| [AesExpert: Towards Multi-modality Foundation Model for Image Aesthetics Perception](https://arxiv.org/abs/2404.09624) (AesExpert, AesMMIT Dataset) | arXiv | 2024-04-15 | [![Star](https://img.shields.io/github/stars/yipoh/AesExpert.svg?style=social&label=Star)](https://github.com/yipoh/AesExpert) | - |
| [Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models](https://arxiv.org/abs/2404.07973) (Ferret-v2) | arXiv | 2024-04-11 | - | - |
| [MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies](https://arxiv.org/abs/2404.06395) (MiniCPM series) | arXiv | 2024-04-09 | [![Star](https://img.shields.io/github/stars/OpenBMB/MiniCPM.svg?style=social&label=Star)](https://github.com/OpenBMB/MiniCPM)<br>[![Star](https://img.shields.io/github/stars/OpenBMB/MiniCPM-V.svg?style=social&label=Star)](https://github.com/OpenBMB/MiniCPM-V) | [![Blog](https://img.shields.io/badge/Technical-Blog-orange.svg)](https://openbmb.vercel.app/?category=Chinese+Blog) |
| [Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs](https://arxiv.org/abs/2404.05719) (Ferret-UI) | arXiv | 2024-04-08 | - | - |
| [MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding](https://arxiv.org/abs/2404.05726) | CVPR 2024 | 2024-04-08 | [![Star](https://img.shields.io/github/stars/boheumd/MA-LMM.svg?style=social&label=Star)](https://github.com/boheumd/MA-LMM) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://boheumd.github.io/MA-LMM/) |
| [Koala: Key frame-conditioned long video-LLM](https://arxiv.org/abs/2404.04346) | CVPR 2024 | 2024-04-05 | [![Star](https://img.shields.io/github/stars/rxtan2/Koala-video-llm.svg?style=social&label=Star)](https://github.com/rxtan2/Koala-video-llm) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://cs-people.bu.edu/rxtan/projects/Koala/) |
| [MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens](https://arxiv.org/abs/2404.03413) | arXiv | 2024-04-04 | [![Star](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT4-video.svg?style=social&label=Star)](https://github.com/Vision-CAIR/MiniGPT4-video) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://vision-cair.github.io/MiniGPT4-video/) |
| [LongVLM: Efficient Long Video Understanding via Large Language Models](https://arxiv.org/abs/2404.03384) | arXiv | 2024-04-04 | [![Star](https://img.shields.io/github/stars/ziplab/LongVLM.svg?style=social&label=Star)](https://github.com/ziplab/LongVLM) | - |
| [MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training](https://arxiv.org/abs/2403.09611) | arXiv | 2024-03-14 | - | - |
| [UniCode: Learning a Unified Codebook for Multimodal Large Language Models](https://arxiv.org/abs/2403.09072) | arXiv | 2024-03-14 | - | - |
| [Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://arxiv.org/abs/2403.05530) | arXiv | 2024-03-08 | - | [![Project Page](https://img.shields.io/badge/Google-Gemini-blue.svg)](https://gemini.google.com/) |
| [Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models](https://arxiv.org/abs/2403.03003) | arXiv | 2023-03-05 | [![Star](https://img.shields.io/github/stars/luogen1996/LLaVA-HR.svg?style=social&label=Star)](https://github.com/luogen1996/LLaVA-HR) | - |
| [RegionGPT: Towards Region Understanding Vision Language Model](https://arxiv.org/abs/2403.02330) | CVPR 2024 | 2024-03-04 | - | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://guoqiushan.github.io/regiongpt.github.io/) |
| [All in an Aggregated Image for In-Image Learning](https://arxiv.org/abs/2402.17971) | arXiv | 2024-02-28 | [![Star](https://img.shields.io/github/stars/AGI-Edgerunners/IIL.svg?style=social&label=Star)](https://github.com/AGI-Edgerunners/IIL) | - |
| [Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners](https://arxiv.org/abs/2402.17723) | CVPR 2024 | 2024-02-27 | [![Star](https://img.shields.io/github/stars/yzxing87/Seeing-and-Hearing.svg?style=social&label=Star)](https://github.com/yzxing87/Seeing-and-Hearing) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://yzxing87.github.io/Seeing-and-Hearing/) |
| [TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages](https://arxiv.org/abs/2402.16021) | arXiv | 2024-02-25 | - | - |
| [LLMBind: A Unified Modality-Task Integration Framework](https://arxiv.org/abs/2402.14891) | arXiv | 2024-02-22 | - | - |
| [AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling](https://arxiv.org/abs/2402.12226) | arXiv | 2024-02-19 | [![Star](https://img.shields.io/github/stars/OpenMOSS/AnyGPT.svg?style=social&label=Star)](https://github.com/OpenMOSS/AnyGPT) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://junzhan2000.github.io/AnyGPT.github.io/) |
| [ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model](https://arxiv.org/abs/2402.11684) (ALLaVA) | arXiv | 2024-02-18 | [![Star](https://img.shields.io/github/stars/FreedomIntelligence/ALLaVA.svg?style=social&label=Star)](https://github.com/FreedomIntelligence/ALLaVA) | [![Demo Page](https://img.shields.io/badge/Demo-Page-purple.svg)](https://allava.freedomai.cn/#/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/FreedomIntelligence/ALLaVA-4V) |
| [MobileVLM V2: Faster and Stronger Baseline for Vision Language Model](https://arxiv.org/abs/2402.03766) | arXiv | 2024-02-06 | [![Star](https://img.shields.io/github/stars/Meituan-AutoML/MobileVLM.svg?style=social&label=Star)](https://github.com/Meituan-AutoML/MobileVLM) | - |
| [Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization](https://arxiv.org/abs/2402.03161) | arXiv | 2024-02-05 | [![Star](https://img.shields.io/github/stars/jy0205/LaVIT.svg?style=social&label=Star)](https://github.com/jy0205/LaVIT) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://video-lavit.github.io/) |
| [Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action](https://arxiv.org/abs/2312.17172) | arXiv | 2023-12-28 | [![Star](https://img.shields.io/github/stars/allenai/unified-io-2.svg?style=social&label=Star)](https://github.com/allenai/unified-io-2) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://unified-io-2.allenai.org/) |
| [MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices](https://arxiv.org/abs/2312.16886) | arXiv | 2023-12-28 | [![Star](https://img.shields.io/github/stars/Meituan-AutoML/MobileVLM.svg?style=social&label=Star)](https://github.com/Meituan-AutoML/MobileVLM) | - |
| [Generative Multimodal Models are In-Context Learners](https://arxiv.org/abs/2312.13286) (Emu2) | CVPR 2024 | 2023-12-20 | [![Star](https://img.shields.io/github/stars/baaivision/Emu.svg?style=social&label=Star)](https://github.com/baaivision/Emu) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://baaivision.github.io/emu2/) |
| [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805) | arXiv | 2023-12-19 | - | [![Project Page](https://img.shields.io/badge/Google-Gemini-blue.svg)](https://gemini.google.com/) |
| [Osprey: Pixel Understanding with Visual Instruction Tuning](https://arxiv.org/abs/2312.10032) | CVPR 2024 | 2023-12-15 | [![Star](https://img.shields.io/github/stars/CircleRadon/Osprey.svg?style=social&label=Star)](https://github.com/CircleRadon/Osprey) | - |
| [VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation](https://arxiv.org/abs/2312.09251) | arXiv | 2023-12-14 | [![Star](https://img.shields.io/github/stars/AILab-CVC/VL-GPT.svg?style=social&label=Star)](https://github.com/AILab-CVC/VL-GPT) | - |
| [Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models](https://arxiv.org/abs/2312.06109) | arXiv | 2023-12-11 | [![Star](https://img.shields.io/github/stars/Ucas-HaoranWei/Vary.svg?style=social&label=Star)](https://github.com/Ucas-HaoranWei/Vary) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://varybase.github.io/) |
| [Prompt Highlighter: Interactive Control for Multi-Modal LLMs](https://arxiv.org/abs/2312.04302) | CVPR 2024 | 2023-12-07 | [![Star](https://img.shields.io/github/stars/dvlab-research/Prompt-Highlighter.svg?style=social&label=Star)](https://github.com/dvlab-research/Prompt-Highlighter) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://julianjuaner.github.io/projects/PromptHighlighter/) |
| [PixelLM: Pixel Reasoning with Large Multimodal Model](https://arxiv.org/abs/2312.02228) | CVPR 2024 | 2023-12-04 | [![Star](https://img.shields.io/github/stars/MaverickRen/PixelLM.svg?style=social&label=Star)](https://github.com/MaverickRen/PixelLM) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://pixellm.github.io/) |
| [APoLLo : Unified Adapter and Prompt Learning for Vision Language Models](https://aclanthology.org/2023.emnlp-main.629/) | EMNLP 2023 | 2023-12-04 | [![Star](https://img.shields.io/github/stars/schowdhury671/APoLLo.svg?style=social&label=Star)](https://github.com/schowdhury671/APoLLo) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://gamma.umd.edu/pro/vision_language/apollo/) |
| [CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation](https://arxiv.org/abs/2311.18775) | arXiv | 2023-11-30 | [![Star](https://img.shields.io/github/stars/microsoft/i-Code.svg?style=social&label=Star)](https://github.com/microsoft/i-Code/tree/main/CoDi-2) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://codi-2.github.io/) |
| [LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](https://arxiv.org/abs/2311.17043) | arXiv | 2023-11-28 | [![Star](https://img.shields.io/github/stars/dvlab-research/LLaMA-VID.svg?style=social&label=Star)](https://github.com/dvlab-research/LLaMA-VID) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://llama-vid.github.io/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/YanweiLi/LLaMA-VID-Data) |
| [LLMGA: Multimodal Large Language Model based Generation Assistant](https://arxiv.org/abs/2311.16500) | arXiv | 2023-11-27 | [![Star](https://img.shields.io/github/stars/dvlab-research/LLMGA.svg?style=social&label=Star)](https://github.com/dvlab-research/LLMGA) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://llmga.github.io/) |
| [PG-Video-LLaVA: Pixel Grounding Large Video-Language Models](https://arxiv.org/abs/2311.13435) | arXiv | 2023-11-22 | [![Star](https://img.shields.io/github/stars/mbzuai-oryx/Video-LLaVA.svg?style=social&label=Star)](https://github.com/mbzuai-oryx/Video-LLaVA) | - |
| [ShareGPT4V: Improving Large Multi-Modal Models with Better Captions](https://arxiv.org/abs/2311.12793) | arXiv | 2023-11-21 | [![Star](https://img.shields.io/github/stars/InternLM/InternLM-XComposer.svg?style=social&label=Star)](https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://sharegpt4v.github.io/) |
| [LION : Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge](https://arxiv.org/abs/2311.11860) | CVPR 2024 | 2023-11-20 | [![Star](https://img.shields.io/github/stars/rshaojimmy/JiuTian.svg?style=social&label=Star)](https://github.com/rshaojimmy/JiuTian) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://rshaojimmy.github.io/Projects/JiuTian-LION) |
| [Video-LLaVA: Learning United Visual Representation by Alignment Before Projection](https://arxiv.org/abs/2311.10122) | arXiv | 2023-11-16 | [![Star](https://img.shields.io/github/stars/PKU-YuanGroup/Video-LLaVA.svg?style=social&label=Star)](https://github.com/PKU-YuanGroup/Video-LLaVA) | - |
| [mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration](https://arxiv.org/abs/2311.04257) | arXiv | 2023-11-07 | [![Star](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl.svg?style=social&label=Star)](https://github.com/X-PLUG/mPLUG-Owl) | - |
| [MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning](https://arxiv.org/abs/2310.09478) | arXiv | 2023-10-14 | [![Star](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4.svg?style=social&label=Star)](https://github.com/Vision-CAIR/MiniGPT-4) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://minigpt-v2.github.io/) |
| [EasyGen: Easing Multimodal Generation with a Bidirectional Conditional Diffusion Model and LLMs](https://arxiv.org/abs/2310.08949) | arXiv | 2023-10-13 | [![Star](https://img.shields.io/github/stars/zxy556677/EasyGen.svg?style=social&label=Star)](https://github.com/zxy556677/EasyGen) | - |
| [Ferret: Refer and Ground Anything Anywhere at Any Granularity](https://arxiv.org/abs/2310.07704) (Ferret) | ICLR 2024 | 2023-10-11 | [![Star](https://img.shields.io/github/stars/apple/ml-ferret.svg?style=social&label=Star)](https://github.com/apple/ml-ferret) | - |
| [Mini-DALLE3: Interactive Text to Image by Prompting Large Language Models](https://arxiv.org/abs/2310.07653) | arXiv | 2023-10-11 | [![Star](https://img.shields.io/github/stars/Zeqiang-Lai/Mini-DALLE3.svg?style=social&label=Star)](https://github.com/Zeqiang-Lai/Mini-DALLE3) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://minidalle3.github.io/) |
| [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/abs/2310.03744) (LLaVA-1.5) | arXiv | 2023-10-05 | [![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star)](https://github.com/haotian-liu/LLaVA) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://llava-vl.github.io/) |
| [Kosmos-G: Generating Images in Context with Multimodal Large Language Models](https://arxiv.org/abs/2310.02992) | ICLR 2024 | 2023-10-04 | [![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)](https://github.com/microsoft/unilm/tree/master/kosmos-g) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://xichenpan.com/kosmosg/) |
| [MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens](https://arxiv.org/abs/2310.02239) | arXiv | 2023-10-03 | [![Star](https://img.shields.io/github/stars/eric-ai-lab/MiniGPT-5.svg?style=social&label=Star)](https://github.com/eric-ai-lab/MiniGPT-5) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://eric-ai-lab.github.io/minigpt-5.github.io/) |
| [Aligning Large Multimodal Models with Factually Augmented RLHF](https://arxiv.org/abs/2309.14525) (LLaVA-RLHF, MMHal-Bench (hallucination)) | arXiv | 2023-09-25 | [![Star](https://img.shields.io/github/stars/llava-rlhf/LLaVA-RLHF.svg?style=social&label=Star)](https://github.com/llava-rlhf/LLaVA-RLHF) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://llava-rlhf.github.io/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/Shengcao1006/MMHal-Bench) |
| [DreamLLM: Synergistic Multimodal Comprehension and Creation](https://arxiv.org/abs/2309.11499) | ICLR 2024 | 2023-09-20 | [![Star](https://img.shields.io/github/stars/RunpeiDong/DreamLLM.svg?style=social&label=Star)](https://github.com/RunpeiDong/DreamLLM) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://dreamllm.github.io/) |
| [MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning](https://arxiv.org/abs/2309.07915) | ICLR 2024 | 2023-09-14 | [![Star](https://img.shields.io/github/stars/HaozheZhao/MIC.svg?style=social&label=Star)](https://github.com/HaozheZhao/MIC) | - |
| [NExT-GPT: Any-to-Any Multimodal LLM](https://arxiv.org/abs/2309.05519) | arXiv | 2023-09-11 | [![Star](https://img.shields.io/github/stars/NExT-GPT/NExT-GPT.svg?style=social&label=Star)](https://github.com/NExT-GPT/NExT-GPT) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://next-gpt.github.io/) |
| [Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization](https://arxiv.org/abs/2309.04669) (LaVIT) | ICLR 2024 | 2023-09-09 | [![Star](https://img.shields.io/github/stars/jy0205/LaVIT.svg?style=social&label=Star)](https://github.com/jy0205/LaVIT) | - |
| [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://arxiv.org/abs/2308.12966) | arXiv | 2023-08-24 | [![Star](https://img.shields.io/github/stars/QwenLM/Qwen-VL.svg?style=social&label=Star)](https://github.com/QwenLM/Qwen-VL) | [![Project Page](https://img.shields.io/badge/阿里-通义千问-blue.svg)](https://tongyi.aliyun.com/qianwen/) |
| [Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages](https://arxiv.org/abs/2308.12038) (VisCPM-Chat/Paint) | ICLR 2024 | 2023-08-23 | [![Star](https://img.shields.io/github/stars/OpenBMB/VisCPM.svg?style=social&label=Star)](https://github.com/OpenBMB/VisCPM) | - |
| [Planting a SEED of Vision in Large Language Model](https://arxiv.org/abs/2307.08041) | ICLR 2024 | 2023-07-16 | [![Star](https://img.shields.io/github/stars/AILab-CVC/SEED.svg?style=social&label=Star)](https://github.com/AILab-CVC/SEED) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://ailab-cvc.github.io/seed/) |
| [Generative Pretraining in Multimodality](https://arxiv.org/abs/2307.05222) (Emu1) | ICLR 2024 | 2023-07-11 | [![Star](https://img.shields.io/github/stars/baaivision/Emu.svg?style=social&label=Star)](https://github.com/baaivision/Emu) | - |
| [SVIT: Scaling up Visual Instruction Tuning](https://arxiv.org/abs/2307.04087) | arXiv | 2023-07-09 | [![Star](https://img.shields.io/github/stars/BAAI-DCAI/Visual-Instruction-Tuning.svg?style=social&label=Star)](https://github.com/BAAI-DCAI/Visual-Instruction-Tuning) | [![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/BAAI/SVIT) |
| [Kosmos-2: Grounding Multimodal Large Language Models to the World](https://arxiv.org/abs/2306.14824) (Kosmos-2, GrIT Dataset) | arXiv | 2023-06-26 | [![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)](https://github.com/microsoft/unilm/tree/master/kosmos-2) | [![Demo](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/ydshieh/Kosmos-2)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/zzliang/GRIT) |
| [M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning](https://arxiv.org/abs/2306.04387) | arXiv | 2023-06-07 | - | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://m3-it.github.io/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/MMInstruction/M3IT) |
| [Generating Images with Multimodal Language Models](https://arxiv.org/abs/2305.17216) (GILL) | NeurIPS 2023 | 2023-05-26 | [![Star](https://img.shields.io/github/stars/kohjingyu/gill.svg?style=social&label=Star)](https://github.com/kohjingyu/gill) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://jykoh.com/gill) |
| [Any-to-Any Generation via Composable Diffusion](https://proceedings.neurips.cc/paper_files/paper/2023/hash/33edf072fe44f19079d66713a1831550-Abstract-Conference.html) (CoDi-1) | NeurIPS 2023 | 2023-05-19 | [![Star](https://img.shields.io/github/stars/microsoft/i-Code.svg?style=social&label=Star)](https://github.com/microsoft/i-Code/tree/main/i-Code-V3) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://codi-gen.github.io/) |
| [SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities](https://aclanthology.org/2023.findings-emnlp.1055/) | EMNLP 2023 (Findings) | 2023-05-18 | [![Star](https://img.shields.io/github/stars/0nutation/SpeechGPT.svg?style=social&label=Star)](https://github.com/0nutation/SpeechGPT) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://0nutation.github.io/SpeechGPT.github.io/) |
| [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://proceedings.neurips.cc/paper_files/paper/2023/hash/9a6a435e75419a836fe47ab6793623e6-Abstract-Conference.html) | NeurIPS 2023 | 2023-05-11 | [![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star)](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip) | - |
| [MultiModal-GPT: A Vision and Language Model for Dialogue with Humans](https://arxiv.org/abs/2305.04790) | arXiv | 2023-05-08 | [![Star](https://img.shields.io/github/stars/open-mmlab/Multimodal-GPT.svg?style=social&label=Star)](https://github.com/open-mmlab/Multimodal-GPT) | - |
| [VPGTrans: Transfer Visual Prompt Generator across LLMs](https://arxiv.org/abs/2305.01278) | NeurIPS 2023 | 2023-05-02 | [![Star](https://img.shields.io/github/stars/VPGTrans/VPGTrans.svg?style=social&label=Star)](https://github.com/VPGTrans/VPGTrans) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://vpgtrans.github.io/) |
| [mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality](https://arxiv.org/abs/2304.14178) | arXiv | 2023-04-27 | [![Star](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl.svg?style=social&label=Star)](https://github.com/X-PLUG/mPLUG-Owl) | - |
| [MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models](https://arxiv.org/abs/2304.10592) | ICLR 2024 | 2023-04-20 | [![Star](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4.svg?style=social&label=Star)](https://github.com/Vision-CAIR/MiniGPT-4) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://minigpt-4.github.io/) |
| [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) (LLaVA) | NeurIPS 2023 | 2023-04-17 | [![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star)](https://github.com/haotian-liu/LLaVA) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://llava-vl.github.io/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K) |
| [Language Is Not All You Need: Aligning Perception with Language Models](https://proceedings.neurips.cc/paper_files/paper/2023/hash/e425b75bac5742a008d643826428787c-Abstract-Conference.html) (Kosmos-1) | NeurIPS 2023 | 2023-02-27 | [![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)](https://github.com/microsoft/unilm) | - |
| [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923) | arXiv | 2023-02-02 | [![Star](https://img.shields.io/github/stars/amazon-science/mm-cot.svg?style=social&label=Star)](https://github.com/amazon-science/mm-cot) | - |
| [Grounding Language Models to Images for Multimodal Inputs and Outputs](https://arxiv.org/abs/2301.13823) (FROMAGe) | ICML 2023 | 2023-01-31 | [![Star](https://img.shields.io/github/stars/kohjingyu/fromage.svg?style=social&label=Star)](https://github.com/kohjingyu/fromage) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://jykoh.com/fromage) |
| [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) | ICML 2023 | 2023-01-30 | [![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star)](https://github.com/salesforce/LAVIS/tree/main/projects/blip2) | - |
| [Flamingo: a Visual Language Model for Few-Shot Learning](https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html) | NeurIPS 2022 | 2022-04-29 | [![Star](https://img.shields.io/github/stars/mlfoundations/open_flamingo.svg?style=social&label=Star)](https://github.com/mlfoundations/open_flamingo) | -|

### LMM Benchmark

|  Title  |   Venue  |   Date   |   Code   |Supplement|
|:--------|:--------:|:--------:|:--------:|:--------:|
| [BLINK: Multimodal Large Language Models Can See but Not Perceive](https://arxiv.org/abs/2404.12390) | arXiv | 2024-04-18 | [![Star](https://img.shields.io/github/stars/zeyofu/BLINK_Benchmark.svg?style=social&label=Star)](https://github.com/zeyofu/BLINK_Benchmark) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://zeyofu.github.io/blink/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/BLINK-Benchmark/BLINK) |
| [Ferret: Refer and Ground Anything Anywhere at Any Granularity](https://arxiv.org/abs/2310.07704) (Ferret-Bench) | ICLR 2024 | 2023-10-11 | [![Star](https://img.shields.io/github/stars/apple/ml-ferret.svg?style=social&label=Star)](https://github.com/apple/ml-ferret) | - |
| [Aligning Large Multimodal Models with Factually Augmented RLHF](https://arxiv.org/abs/2309.14525) (LLaVA-RLHF, MMHal-Bench (hallucination)) | arXiv | 2023-09-25 | [![Star](https://img.shields.io/github/stars/llava-rlhf/LLaVA-RLHF.svg?style=social&label=Star)](https://github.com/llava-rlhf/LLaVA-RLHF) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://llava-rlhf.github.io/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/Shengcao1006/MMHal-Bench) |
| [SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension](https://arxiv.org/abs/2307.16125) | CVPR 2024 | 2023-07-30 | [![Star](https://img.shields.io/github/stars/AILab-CVC/SEED-Bench.svg?style=social&label=Star)](https://github.com/AILab-CVC/SEED-Bench) | - |

### Multimodal Dialogue

|  Title  |   Venue  |   Date   |   Code   |Supplement|
|:--------|:--------:|:--------:|:--------:|:--------:|
| [DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation](https://arxiv.org/abs/2403.08857) | arXiv | 2024-03-13 | [![Star](https://img.shields.io/github/stars/Centaurusalpha/DialogGen.svg?style=social&label=Star)](https://github.com/Centaurusalpha/DialogGen) | - |
| [STICKERCONV: Generating Multimodal Empathetic Responses from Scratch](https://arxiv.org/abs/2402.01679) | arXiv | 2024-01-20 | [![Star](https://img.shields.io/github/stars/ZhangYiqun018/StickerConv.svg?style=social&label=Star)](https://github.com/ZhangYiqun018/StickerConv) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://neu-datamining.github.io/StickerConv/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/NEUDM/StickerConv) |
| [VDialogUE: A Unified Evaluation Benchmark for Visually-grounded Dialogue](https://arxiv.org/abs/2309.07387) | arXiv | 2023-09-14 | - | - |
| [PaCE: Unified Multi-modal Dialogue Pre-training with Progressive and Compositional Experts](https://aclanthology.org/2023.acl-long.749/) | ACL 2023 | 2023-05-24 | [![Star](https://img.shields.io/github/stars/AlibabaResearch.svg?style=social&label=Star)](https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/pace) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://ruc-aimind.github.io/projects/TikTalk/) |
| [TikTalk: A Multi-Modal Dialogue Dataset for Real-World Chitchat](https://arxiv.org/abs/2301.05880) | ACM MM 2023 | 2023-01-14 | [![Star](https://img.shields.io/github/stars/RUC-AIMind/TikTalk.svg?style=social&label=Star)](https://github.com/RUC-AIMind/TikTalk) | Dataset |
| [MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation](https://aclanthology.org/2023.acl-long.405/) | ACL 2023 | 2022-11-10 | [![Star](https://img.shields.io/github/stars/victorsungo/MMDialog.svg?style=social&label=Star)](https://github.com/victorsungo/MMDialog) | Dataset |
| [Multimodal Dialogue Response Generation](https://aclanthology.org/2022.acl-long.204/) (Divter) | ACL 2022 | 2021-10-16 | - | - |
| [Maria: A Visual Experience Powered Conversational Agent](https://aclanthology.org/2021.acl-long.435/) | ACL 2021 | 2021-05-27 | [![Star](https://img.shields.io/github/stars/jokieleung/Maria.svg?style=social&label=Star)](https://github.com/jokieleung/Maria) | - |
| [Multi-Modal Open-Domain Dialogue](https://aclanthology.org/2021.emnlp-main.398/) | EMNLP 2021 | 2020-10-02 | - | - |
| [Open Domain Dialogue Generation with Latent Images](https://arxiv.org/abs/2004.01981) | AAAI 2021 | 2020-04-04 | - | - |
| [Learning to Respond with Stickers: A Framework of Unifying Multi-Modality in Multi-Turn Dialog](https://arxiv.org/abs/2003.04679) | WWW 2020 | 2020-03-10 | [![Star](https://img.shields.io/github/stars/gsh199449/stickerchat.svg?style=social&label=Star)](https://github.com/gsh199449/stickerchat) | - |

### Multimodal Learning

|  Title  |   Venue  |   Date   |   Code   |Supplement|
|:--------|:--------:|:--------:|:--------:|:--------:|
| [Video as the New Language for Real-World Decision Making](https://arxiv.org/abs/2402.17139) | arXiv | 2024-02-27 | - | - |
| [Tokenize Anything via Prompting](https://arxiv.org/abs/2312.09128) | arXiv | 2023-12-14 | [![Star](https://img.shields.io/github/stars/baaivision/tokenize-anything.svg?style=social&label=Star)](https://github.com/baaivision/tokenize-anything) | - |
| [LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment](https://arxiv.org/abs/2310.01852) | ICLR 2024 | 2023-10-03 | [![Star](https://img.shields.io/github/stars/PKU-YuanGroup/LanguageBind.svg?style=social&label=Star)](https://github.com/PKU-YuanGroup/LanguageBind) | - |
| [ImageBind: One Embedding Space To Bind Them All](https://openaccess.thecvf.com/content/CVPR2023/html/Girdhar_ImageBind_One_Embedding_Space_To_Bind_Them_All_CVPR_2023_paper.html) | CVPR 2023 | 2023-05-09 | [![Star](https://img.shields.io/github/stars/facebookresearch/ImageBind.svg?style=social&label=Star)](https://github.com/facebookresearch/ImageBind) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://imagebind.metademolab.com/) |
| [Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks](https://openaccess.thecvf.com/content/CVPR2023/html/Li_Uni-Perceiver_v2_A_Generalist_Model_for_Large-Scale_Vision_and_Vision-Language_CVPR_2023_paper.html) | CVPR 2023 | 2022-11-17 | [![Star](https://img.shields.io/github/stars/fundamentalvision/Uni-Perceiver.svg?style=social&label=Star)](https://github.com/fundamentalvision/Uni-Perceiver) | - |
| [Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks](https://arxiv.org/abs/2208.10442) (BEiT-3) | CVPR 2023 | 2022-08-22 | [![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)](https://github.com/microsoft/unilm/tree/master/beit3) | - |
| [BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers](https://arxiv.org/abs/2208.06366) | arXiv | 2022-08-12 | [![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)](https://github.com/microsoft/unilm/tree/master/beit2) | - |
| [BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning](https://arxiv.org/abs/2206.08657) | AAAI 2023 | 2022-06-17 | [![Star](https://img.shields.io/github/stars/microsoft/BridgeTower.svg?style=social&label=Star)](https://github.com/microsoft/BridgeTower) | [![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Transformers-blue)](https://huggingface.co/docs/transformers/model_doc/bridgetower) |
| [OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework](https://arxiv.org/abs/2202.03052) | ICML 2022 | 2022-02-07 | [![Star](https://img.shields.io/github/stars/OFA-Sys/OFA.svg?style=social&label=Star)](https://github.com/OFA-Sys/OFA) | - |
| [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) | ICML 2022 | 2022-01-28 | [![Star](https://img.shields.io/github/stars/salesforce/BLIP.svg?style=social&label=Star)](https://github.com/salesforce/BLIP) | - |
| [Uni-Perceiver: Pre-Training Unified Architecture for Generic Perception for Zero-Shot and Few-Shot Tasks](https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Uni-Perceiver_Pre-Training_Unified_Architecture_for_Generic_Perception_for_Zero-Shot_and_CVPR_2022_paper.html) | CVPR 2022 | 2021-12-02 | [![Star](https://img.shields.io/github/stars/fundamentalvision/Uni-Perceiver.svg?style=social&label=Star)](https://github.com/fundamentalvision/Uni-Perceiver) | - |
| [Align before Fuse: Vision and Language Representation Learning with Momentum Distillation](https://arxiv.org/abs/2107.07651) (ALBEF) | NeurIPS 2021 | 2021-07-16 | [![Star](https://img.shields.io/github/stars/salesforce/ALBEF.svg?style=social&label=Star)](https://github.com/salesforce/ALBEF) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://blog.salesforceairesearch.com/align-before-fuse/) |
| [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) | ICLR 2022 | 2021-06-15 | [![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)](https://github.com/microsoft/unilm/tree/master/beit) | - |

### Image-to-Text Generation

|  Title  |   Venue  |   Date   |   Code   |Supplement|
|:--------|:--------:|:--------:|:--------:|:--------:|
| [LaDiC: Are Diffusion Models Really Inferior to Autoregressive Counterparts for Image-to-Text Generation?](https://arxiv.org/abs/2404.10763) (LaDiC) | NAACL 2024 | 2024-04-16 | [![Star](https://img.shields.io/github/stars/wangyuchi369/LaDiC.svg?style=social&label=Star)](https://github.com/wangyuchi369/LaDiC) | - |

### Text/Image-to-Image Generation

|  Title  |   Venue  |   Date   |   Code   |Supplement|
|:--------|:--------:|:--------:|:--------:|:--------:|
| [FreeU: Free Lunch in Diffusion U-Net](https://arxiv.org/abs/2309.11497) (FreeU, by Ziwei Liu) | CVPR 2024 Oral | 2023-09-20 | [![Star](https://img.shields.io/github/stars/ChenyangSi/FreeU.svg?style=social&label=Star)](https://github.com/ChenyangSi/FreeU) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://chenyangsi.top/FreeU/)<br>[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/-CZ5uWxvX30)<br>[![Demo](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/ChenyangSi/FreeU) |
| [Lazy Diffusion Transformer for Interactive Image Editing](https://arxiv.org/abs/2404.12382) | arXiv | 2024-04-18 | - | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://lazydiffusion.github.io/) |
| [Salient Object-Aware Background Generation using Text-Guided Diffusion Models](https://arxiv.org/abs/2404.10157) | CVPR 2024 Workshop | 2024-04-15 | [![Star](https://img.shields.io/github/stars/yahoo/photo-background-generation.svg?style=social&label=Star)](https://github.comyahoo/photo-background-generation/) | - |
| [HQ-Edit: A High-Quality Dataset for Instruction-based Image Editing](https://arxiv.org/abs/2404.09990) | arXiv | 2024-04-15 | [![Star](https://img.shields.io/github/stars/UCSC-VLAA/HQ-Edit.svg?style=social&label=Star)](https://github.com/UCSC-VLAA/HQ-Edit) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://thefllood.github.io/HQEdit_web/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/UCSC-VLAA/HQ-Edit)<br>[![Demo](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/LAOS-Y/HQEdit) |
| [UNIAA: A Unified Multi-modal Image Aesthetic Assessment Baseline and Benchmark](https://arxiv.org/abs/2404.09619) (UNIAA-LLaVA, UNIAA-Bench) | arXiv | 2024-04-15 | - | - |
| [PMG: Personalized Multimodal Generation with Large Language Models](https://arxiv.org/abs/2404.08677) | WWW 2024 | 2024-04-07 | - | - |
| [Identity Decoupling for Multi-Subject Personalization of Text-to-Image Models](https://arxiv.org/abs/2404.04243) | arXiv | 2024-04-05 | [![Star](https://img.shields.io/github/stars/agwmon/MuDI.svg?style=social&label=Star)](https://github.com/agwmon/MuDI) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://mudi-t2i.github.io/) |
| [Concept Weaver: Enabling Multi-Concept Fusion in Text-to-Image Models](https://arxiv.org/abs/2404.03913) | CVPR 2024 | 2024-04-05 | - | - |
| [Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction](https://arxiv.org/abs/2404.02905) (VAR) | arXiv | 2024-04-03 | [![Star](https://img.shields.io/github/stars/FoundationVision/VAR.svg?style=social&label=Star)](https://github.com/FoundationVision/VAR) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://var.vision/) |
| [PixArt-Σ: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation](https://arxiv.org/abs/2403.04692) (HuaWei Enze Xie) | arXiv | 2024-03-07 | [![Star](https://img.shields.io/github/stars/PixArt-alpha/PixArt-sigma.svg?style=social&label=Star)](https://github.com/PixArt-alpha/PixArt-sigma) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://pixart-alpha.github.io/PixArt-sigma-project/) |
| [Multi-LoRA Composition for Image Generation](https://arxiv.org/abs/2402.16843) | arXiv | 2024-02-26 | [![Star](https://img.shields.io/github/stars/maszhongming/Multi-LoRA-Composition.svg?style=social&label=Star)](https://github.com/maszhongming/Multi-LoRA-Composition) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://maszhongming.github.io/Multi-LoRA-Composition/) |
| [PIXART-δ: Fast and Controllable Image Generation with Latent Consistency Models](https://arxiv.org/abs/2401.05252) (HuaWei Enze Xie) | arXiv | 2024-01-10 | [![Star](https://img.shields.io/github/stars/PixArt-alpha/PixArt-alpha.svg?style=social&label=Star)](https://github.com/PixArt-alpha/PixArt-alpha) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://pixart-alpha.github.io/) |
| [Brush Your Text: Synthesize Any Scene Text on Images via Diffusion Model](https://arxiv.org/abs/2312.12232) | AAAI 2024 | 2023-12-19 | [![Star](https://img.shields.io/github/stars/ecnuljzhang/brush-your-text.svg?style=social&label=Star)](https://github.com/ecnuljzhang/brush-your-text) | - |
| [SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models](https://arxiv.org/abs/2312.06739) (Tencent Xintao Wang) | arXiv | 2023-12-11 | [![Star](https://img.shields.io/github/stars/TencentARC/SmartEdit.svg?style=social&label=Star)](https://github.com/TencentARC/SmartEdit) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://yuzhou914.github.io/SmartEdit/) |
| [InstructAny2Pix: Flexible Visual Editing via Multimodal Instruction Following](https://arxiv.org/abs/2312.06738) | arXiv | 2023-12-11 | [![Star](https://img.shields.io/github/stars/jacklishufan/InstructAny2Pix.svg?style=social&label=Star)](https://github.com/jacklishufan/InstructAny2Pix) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://homepage.jackli.org/projects/instructany2pix.html) |
| [Emu Edit: Precise Image Editing via Recognition and Generation Tasks](https://arxiv.org/abs/2311.10089) | arXiv | 2023-11-16 | - | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://emu-edit.metademolab.com/) |
| [BeautifulPrompt: Towards Automatic Prompt Engineering for Text-to-Image Synthesis](https://aclanthology.org/2023.emnlp-industry.1/) | EMNLP 2023 | 2023-11-12 | [![Star](https://img.shields.io/github/stars/alibaba/EasyNLP.svg?style=social&label=Star)](https://github.com/alibaba/EasyNLP) | [![zhihu](https://img.shields.io/badge/-知乎-000000?logo=zhihu&logoColor=0084FF)](https://zhuanlan.zhihu.com/p/636546340) |
| [AnyText: Multilingual Visual Text Generation And Editing](https://arxiv.org/abs/2311.03054) | ICLR 2024 | 2023-11-06 | [![Star](https://img.shields.io/github/stars/tyxsspa/AnyText.svg?style=social&label=Star)](https://github.com/tyxsspa/AnyText) | - |
| [PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis](https://arxiv.org/abs/2310.00426) (HuaWei Enze Xie) | ICLR 2024 Spotlight | 2023-09-30 | [![Star](https://img.shields.io/github/stars/PixArt-alpha/PixArt-alpha.svg?style=social&label=Star)](https://github.com/PixArt-alpha/PixArt-alpha) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://pixart-alpha.github.io/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/PixArt-alpha/SAM-LLaVA-Captions10M)<br>[![Usage Diffusers](https://img.shields.io/badge/Usage-Diffusers-green.svg)](https://huggingface.co/docs/diffusers/main/en/api/pipelines/pixart) |
| [IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models](https://arxiv.org/abs/2308.06721) | arXiv | 2023-08-13 | [![Star](https://img.shields.io/github/stars/tencent-ailab/IP-Adapter.svg?style=social&label=Star)](https://github.com/tencent-ailab/IP-Adapter) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://ip-adapter.github.io/) |
| [Kosmos-G: Generating Images in Context with Multimodal Large Language Models](https://arxiv.org/abs/2310.02992) | arXiv | 2023-10-04 | [![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)](https://github.com/microsoft/unilm/tree/master/kosmos-g) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://xichenpan.com/kosmosg/) |
| [Improving Image Generation with Better Captions](https://cdn.openai.com/papers/dall-e-3.pdf) (DALL-E 3) | OpenAI | 2023 | - | - |
| [Scaling up GANs for Text-to-Image Synthesis](https://arxiv.org/abs/2303.05511) (GigaGAN) | CVPR 2023 | 2023-05-09 | [![Star](https://img.shields.io/github/stars/mingukkang/GigaGAN.svg?style=social&label=Star)](https://github.com/mingukkang/GigaGAN) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://mingukkang.github.io/GigaGAN/) |
| [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543) (ControlNet) | ICCV 2023 | 2023-02-10 | [![Star](https://img.shields.io/github/stars/lllyasviel/ControlNet.svg?style=social&label=Star)](https://github.com/lllyasviel/ControlNet) | - |
| [Scalable Diffusion Models with Transformers](https://arxiv.org/abs/2212.09748) (DiT) | ICCV 2023 | 2022-12-19 | [![Star](https://img.shields.io/github/stars/facebookresearch/DiT.svg?style=social&label=Star)](https://github.com/facebookresearch/DiT) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://www.wpeebles.com/DiT) |
| [InstructPix2Pix: Learning to Follow Image Editing Instructions](https://arxiv.org/abs/2211.09800) | CVPR 2023 | 2022-11-17 | [![Star](https://img.shields.io/github/stars/timothybrooks/instruct-pix2pix.svg?style=social&label=Star)](https://github.com/timothybrooks/instruct-pix2pix) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://www.timothybrooks.com/instruct-pix2pix) |
| [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/abs/2208.12242) | CVPR 2023 | 2022-08-25 | [![Star](https://img.shields.io/github/stars/google/dreambooth.svg?style=social&label=Star)](https://github.com/google/dreambooth) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://dreambooth.github.io/) |
| [Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding](https://arxiv.org/abs/2205.11487) (Imagen) | NeurIPS 2022 | 2022-05-23 | [![Star](https://img.shields.io/github/stars/lucidrains/imagen-pytorch.svg?style=social&label=Star)](https://github.com/lucidrains/imagen-pytorch) | [![Project Page](https://img.shields.io/badge/Google-Imagen-blue.svg)](https://imagen.research.google/) |
| [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125) (DALL-E 2) | OpenAI | 2022-04-13 | [![Star](https://img.shields.io/github/stars/lucidrains/DALLE2-pytorch.svg?style=social&label=Star)](https://github.com/lucidrains/DALLE2-pytorch) | - |
| [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752) (LDM, Stable Diffusion) | CVPR 2022 | 2021-12-20 | [![Star](https://img.shields.io/github/stars/CompVis/latent-diffusion.svg?style=social&label=Star)](https://github.com/CompVis/latent-diffusion) | - |
| [GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models](https://arxiv.org/abs/2112.10741) | ICML 2022 | 2021-12-20 | [![Star](https://img.shields.io/github/stars/openai/glide-text2im.svg?style=social&label=Star)](https://github.com/openai/glide-text2im) | - |
| [NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion](https://arxiv.org/abs/2111.12417) | ECCV 2022 | 2021-11-24 | [![Star](https://img.shields.io/github/stars/microsoft/NUWA.svg?style=social&label=Star)](https://github.com/microsoft/NUWA) | - |
| [SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations](https://arxiv.org/abs/2108.01073) | ICLR 2022 | 2021-08-02 | [![Star](https://img.shields.io/github/stars/ermongroup/SDEdit.svg?style=social&label=Star)](https://github.com/ermongroup/SDEdit) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://sde-image-editing.github.io/) |
| [CogView: Mastering Text-to-Image Generation via Transformers](https://arxiv.org/abs/2105.13290) | NeurIPS 2021 | 2021-05-26 | [![Star](https://img.shields.io/github/stars/THUDM/CogView.svg?style=social&label=Star)](https://github.com/THUDM/CogView) | - |
| [Zero-Shot Text-to-Image Generation](https://arxiv.org/abs/2102.12092) (DALL-E 1) | ICML 2021 | 2021-02-24 | [![Star](https://img.shields.io/github/stars/openai/DALL-E.svg?style=social&label=Star)](https://github.com/openai/DALL-E) | [![Project Page](https://img.shields.io/badge/OpenAI-DALL·E-blue.svg)](https://openai.com/research/dall-e) |
| [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841) (VQ-GAN) | CVPR 2021 | 2020-12-17 | [![Star](https://img.shields.io/github/stars/CompVis/taming-transformers.svg?style=social&label=Star)](https://github.com/CompVis/taming-transformers) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://compvis.github.io/taming-transformers/) |

### Video Generation

|  Title  |   Venue  |   Date   |   Code   |Supplement|
|:--------|:--------:|:--------:|:--------:|:--------:|
| [StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text](https://arxiv.org/abs/2403.14773) (Long Video Generation) | arXiv | 2024-03-21 | [![Star](https://img.shields.io/github/stars/Picsart-AI-Research/StreamingT2V.svg?style=social&label=Star)](https://github.com/Picsart-AI-Research/StreamingT2V) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://streamingt2v.github.io/)<br>[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/GDPP0zmFmQg)<br>[![Demo](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/StreamingT2V)<br>[![Wechat](https://img.shields.io/badge/-WeChat@新智元-000000?logo=wechat&logoColor=07C160)](https://mp.weixin.qq.com/s/X9vz21WrzMwfC3BPSkiCrQ) |
| [AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks](https://arxiv.org/abs/2403.14468) | arXiv | 2024-03-21 | [![Star](https://img.shields.io/github/stars/TIGER-AI-Lab/AnyV2V.svg?style=social&label=Star)](https://github.com/TIGER-AI-Lab/AnyV2V) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://tiger-ai-lab.github.io/AnyV2V/)<br>[![Demo](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/TIGER-Lab/AnyV2V)<br>[![Demo Page](https://img.shields.io/badge/Demo-Page-purple.svg)](https://replicate.com/tiger-ai-lab/anyv2v)<br>[![Wechat](https://img.shields.io/badge/-WeChat@数源AI-000000?logo=wechat&logoColor=07C160)](https://mp.weixin.qq.com/s/shkKypEeNjf_8uQCYDycsg) |
| [FreeInit: Bridging Initialization Gap in Video Diffusion Models](https://arxiv.org/abs/2312.07537) (FreeInit, by Ziwei Liu) | arXiv | 2023-12-12 | [![Star](https://img.shields.io/github/stars/TianxingWu/FreeInit.svg?style=social&label=Star)](https://github.com/TianxingWu/FreeInit) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://tianxingwu.github.io/pages/FreeInit/)<br>[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/lS5IYbAqriI)<br>[![Demo](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/TianxingWu/FreeInit) |
| [FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling](https://arxiv.org/abs/2310.15169) (FreeNoise, by Ziwei Liu) | ICLR 2024 | 2023-10-23 | [![Star](https://img.shields.io/github/stars/AILab-CVC/FreeNoise.svg?style=social&label=Star)](https://github.com/AILab-CVC/FreeNoise) | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](http://haonanqiu.com/projects/FreeNoise.html) |
| [Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets](https://arxiv.org/abs/2311.15127) (SVD) | arXiv | 2023-11-25 | [![Star](https://img.shields.io/github/stars/Stability-AI/generative-models.svg?style=social&label=Star)](https://github.com/Stability-AI/generative-models) | [![Project Page](https://img.shields.io/badge/Stability%20AI-purple.svg)](https://stability.ai/membership) |


### Multimodal Dataset

|  Title  |   Venue  |   Date   |Annotation|  Source  |
|:--------|:--------:|:--------:|:--------:|:--------:|
| [TextSquare: Scaling up Text-Centric Visual Instruction Tuning](https://arxiv.org/abs/2404.12803) | arXiv | 2024-04-19 | Visual Instruction Tuning | - |
| [HQ-Edit: A High-Quality Dataset for Instruction-based Image Editing](https://arxiv.org/abs/2404.09990) | arXiv | 2024-04-15 | Instruction Image Editing | [![Star](https://img.shields.io/github/stars/UCSC-VLAA/HQ-Edit.svg?style=social&label=Star)](https://github.com/UCSC-VLAA/HQ-Edit)<br>[![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://thefllood.github.io/HQEdit_web/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/UCSC-VLAA/HQ-Edit)<br>[![Demo](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/LAOS-Y/HQEdit) |
| [AesExpert: Towards Multi-modality Foundation Model for Image Aesthetics Perception](https://arxiv.org/abs/2404.09624) (AesExpert, AesMMIT Dataset) | arXiv | 2024-04-15 | Aesthetic Multi-Modality Instruction Tuning | [![Star](https://img.shields.io/github/stars/yipoh/AesExpert.svg?style=social&label=Star)](https://github.com/yipoh/AesExpert) |
| [Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers](https://arxiv.org/abs/2402.19479) | CVPR 2024 | 2024-02-29 | video-caption | [![Star](https://img.shields.io/github/stars/snap-research/Panda-70M.svg?style=social&label=Star)](https://github.com/snap-research/Panda-70M)<br>[![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://snap-research.github.io/Panda-70M/) |
| [ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model](https://arxiv.org/abs/2402.11684) | arXiv | 2024-02-18 | GPT4V-synthesized Data | [![Star](https://img.shields.io/github/stars/FreedomIntelligence/ALLaVA.svg?style=social&label=Star)](https://github.com/FreedomIntelligence/ALLaVA)<br>[![Demo Page](https://img.shields.io/badge/Demo-Page-purple.svg)](https://allava.freedomai.cn/#/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/FreedomIntelligence/ALLaVA-4V) |
| [STICKERCONV: Generating Multimodal Empathetic Responses from Scratch](https://arxiv.org/abs/2402.01679) | arXiv | 2024-01-20 | Multimodal Empathetic Dialogue | [![Star](https://img.shields.io/github/stars/ZhangYiqun018/StickerConv.svg?style=social&label=Star)](https://github.com/ZhangYiqun018/StickerConv)<br>[![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://neu-datamining.github.io/StickerConv/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/NEUDM/StickerConv) |
| [SVIT: Scaling up Visual Instruction Tuning](https://arxiv.org/abs/2307.04087) | arXiv | 2023-07-09 | Instruction Tuning | [![Star](https://img.shields.io/github/stars/BAAI-DCAI/Visual-Instruction-Tuning.svg?style=social&label=Star)](https://github.com/BAAI-DCAI/Visual-Instruction-Tuning)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/BAAI/SVIT) |
| [Kosmos-2: Grounding Multimodal Large Language Models to the World](https://arxiv.org/abs/2306.14824) (Kosmos-2, GrIT Dataset) | arXiv | 2023-06-26 | Grounded image-text pairs | [![Star](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&label=Star)](https://github.com/microsoft/unilm/tree/master/kosmos-2)<br>[![Demo](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/ydshieh/Kosmos-2)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/zzliang/GRIT) |
| [M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning](https://arxiv.org/abs/2306.04387) | arXiv | 2023-06-07 | Instruction Tuning | [![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://m3-it.github.io/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/MMInstruction/M3IT) |
| [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) (LLaVA) | NeurIPS 2023 | 2023-04-17 | Instruction Tuning | [![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star)](https://github.com/haotian-liu/LLaVA)<br>[![Project Page](https://img.shields.io/badge/Project-Page-green.svg)](https://llava-vl.github.io/)<br>[![Dataset](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K) |
| [Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text](https://arxiv.org/abs/2304.06939) | NeurIPS D&B 2023 | 2023-04-14 | Interleaved Image-Text | [![Star](https://img.shields.io/github/stars/allenai/mmc4.svg?style=social&label=Star)](https://github.com/allenai/mmc4) |
| [TikTalk: A Multi-Modal Dialogue Dataset for Real-World Chitchat](https://arxiv.org/abs/2301.05880) | ACM MM 2023 | 2023-01-14 | Multimodal Dialogue | [![Star](https://img.shields.io/github/stars/RUC-AIMind/TikTalk.svg?style=social&label=Star)](https://github.com/RUC-AIMind/TikTalk) |
| [MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation](https://aclanthology.org/2023.acl-long.405/) | ACL 2023 | 2022-11-10 | Multimodal Dialogue | [![Star](https://img.shields.io/github/stars/victorsungo/MMDialog.svg?style=social&label=Star)](https://github.com/victorsungo/MMDialog) |
| [LAION-5B: An open large-scale dataset for training next generation image-text models](https://arxiv.org/abs/2210.08402) | NeurIPS 2022 | 2022-10-16 | Image-Text Pairs | [![Project Page](https://img.shields.io/badge/LAION-5B-blue.svg)](https://laion.ai/blog/laion-5b/) |
| [LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs](https://arxiv.org/abs/2111.02114) | NeurIPS Workshop 2021 | 2021-11-03 | Image-Text Pairs | [![Project Page](https://img.shields.io/badge/LAION-400M-blue.svg)](https://laion.ai/blog/laion-400-open-dataset/) |
| [MMConv: An Environment for Multimodal Conversational Search across Multiple Domains](https://dl.acm.org/doi/10.1145/3404835.3462970) | ACM SIGIR 2021 | 2021-07 | Multimodal Dialogue | [![Star](https://img.shields.io/github/stars/liziliao/MMConv.svg?style=social&label=Star)](https://github.com/liziliao/MMConv) |
| [PhotoChat: A Human-Human Dialogue Dataset With Photo Sharing Behavior For Joint Image-Text Modeling](https://aclanthology.org/2021.acl-long.479/) | ACL 2021 | 2021-07-06 | Open-domain Multimodal Dialogue | [![Star](https://img.shields.io/github/stars/google-research/google-research.svg?style=social&label=Star)](https://github.com/google-research/google-research/tree/master/multimodalchat/) |
| [Image-Chat: Engaging Grounded Conversations](https://aclanthology.org/2020.acl-main.219/) | ACL 2020 | 2018-11-02 | Multimodal Dialogue | [![Project Page](https://img.shields.io/badge/ImageChat-blue.svg)](https://parl.ai/projects/image_chat/) |

### Multimodal Summary

|  Title  |   Venue  |   Date   | Latest Update |
|:--------|:--------:|:--------:|:--------:|
| [Theoretical research on generative diffusion models: an overview](https://arxiv.org/abs/2404.09016) | arXiv | 2024-04-13 | - |
| [A Review of Multi-Modal Large Language and Vision Models](https://arxiv.org/abs/2404.01322) | arXiv | 2024-03-28 | - |
| [The (R)Evolution of Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2402.12451) | arXiv | 2024-02-19 | - |
| [MM-LLMs: Recent Advances in MultiModal Large Language Models](https://arxiv.org/abs/2401.13601) | arXiv | 2024-01-24 | 2024-02-20 |
| [Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2311.13165) | 	IEEE BigData 2023 | 2023-11-22 | - |
| [Multimodal Foundation Models: From Specialists to General-Purpose Assistants](https://arxiv.org/abs/2309.10020) | CVPR 2023 | 2023-09-18 | - |
| [Understanding Deep Learning](https://udlbook.github.io/udlbook/) | - | 2023 | - |
| [Large Multimodal Models: Notes on CVPR 2023 Tutorial](https://arxiv.org/abs/2306.14895) | CVPR 2023 | 2023-06-26 | - |
| [A Survey on Multimodal Large Language Models](https://arxiv.org/abs/2306.13549) | arXiv | 2023-06-23 | 2024-04-01 |
| [Multimodal Deep Learning](https://arxiv.org/abs/2301.04856) | arXiv | 2023-01-12 | - |
| [Diffusion Models: A Comprehensive Survey of Methods and Applications](https://arxiv.org/abs/2209.00796) | ACM Computing Surveys | 2022-09-02 | 2024-02-06 |
| [Multimodal Learning with Transformers: A Survey](https://arxiv.org/abs/2206.06488) | IEEE TPAMI 2023 | 2022-01-13<br> |2023-05-10|
| [Multimodal Machine Learning: A Survey and Taxonomy](https://ieeexplore.ieee.org/document/8269806) | IEEE PAMI 2019 | 2017-05-26 | 2017-08-01 |

